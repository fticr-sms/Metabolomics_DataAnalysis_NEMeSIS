{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NEMeSIS","text":"<p>NEMeSIS (Novel Extreme-resolution Metabolomics Streamlined data analysis Integrated Software) is a software geared towards FT-ICR-MS extreme-resolution mass spectrometry data. For this objective, it offers a complete pipeline from raw data (in mzML format) or peak lists to data interpretation. It exists as a panel and param-based (Python packages) graphical interface. Furthermore, it also encompasses illustrative jupyter notebooks to show how analysis can also be performed programatically.</p> <ul> <li> <p>Graphical Interface provides a streamlined experience where no coding knowledge is necessary and results are shown in an interactive format. The file to start to experience NEMeSIS in this way is <code>Interface_NEMeSIS.py</code>. Detailed instructions and ways to start up the program are in the Get started with GUI page.</p> </li> <li> <p>Illustrative Jupyter Notebook for a more hands on approach for intermediate (or higher) experienced Python users. After familiarizing themselves, users can take advantage of the implemented workflow and customize it to their liking. The file to experience NEMeSIS in this way is <code>NEMeSIS_Data_Analysis_V2.8.ipynb</code> - see details on Get started with jupyter page.</p> </li> </ul> <p>Here, on this page, we will detail on how to install this software and dependencies to run them. Then, we will have a separate page for each main way to use NEMeSIS with a tutorial on how to use them.</p> <p>Software made by: FT-ICR and Structural Mass Spectrometry Laboratory, BioISI, Faculdade de Ci\u00eancias da Universidade de Lisboa (http://ft-icr.rd.ciencias.ulisboa.pt/).</p> <p>If you use this software in your untargeted metabolomcis data analysis, we would be grateful if you cite our paper introducing it:</p> <ul> <li>Paper Link: (LINK)</li> <li>Citation: (CITATION)</li> </ul>"},{"location":"#installing-nemesis","title":"Installing NEMeSIS","text":"<p>NEMeSIS is currently available as an open source software that can be downloaded or cloned from https://github.com/fticr-sms/Metabolomics_DataAnalysis_NEMeSIS. This includes a <code>venn.py</code> file originally provided in https://github.com/tctianchi/pyvenn/blob/master/venn.py and used according to its license. It is a Python-based software and does requires the Python programming language and has many dependencies on different Python packages.</p>"},{"location":"#installing-python-and-other-packages-with-anaconda","title":"Installing Python and other packages with Anaconda","text":"<p>We recommend the installation of Anaconda to use the software, with a tutorial available at https://docs.anaconda.com/anaconda/install/index.html.</p> <p>From here, we present two ways to install the remaining packages needed, the first one package-by-package and the second which will install every package rapidly (requirements.txt based).</p>"},{"location":"#installing-by-creating-a-dedicated-python-environment-nemesis_env","title":"Installing by creating a dedicated Python environment - NEMeSIS_env","text":"<p>This strategy will install a virtual environment called <code>NEMeSIS_env</code> on your computer with all necessary packages to run NEMeSIS both as a notebook and as a graphical interface. You will need to change the selected environment once before either running the jupyter notebook (change kernel running to NEMeSIS_env) or the graphical interface (run <code>conda activate NEMeSIS_env</code> before opening the interface - explained in Get started with GUI)</p> <p>Using the provided NEMeSIS.yml file</p> <p>On the command line (or the AnacondaPrompt in case Anaconda was not added yo your PATH during installation) in your pc, run the following 2 code lines (one at a time):</p> <pre><code>conda env create -f NEMeSIS.yml\n\npython -m ipykernel install --user --name=NEMeSIS_env\n</code></pre> <p>Warning</p> <p>On Non-Windows machines, the environment installation may become incomplete due to the kaleido package. This is due to the kaleido version 0.1.0.post1 installed not existing for Non-Windows installations. For these cases, try to change the version present in the yml or the requirements file to 0.1.0 or 0.2.0 before installing the environment. Kaleido versions will affect downloading the interactive figures from the graphical interface.</p> <p>Using the provided requirements.txt file</p> <p>Alternatively, you may also install the environment from the <code>requirements.txt</code> file with the following lines:</p> <pre><code>conda create -n NEMeSIS_env --file requirements.txt\n\npython -m ipykernel install --user --name=NEMeSIS_env\n</code></pre>"},{"location":"#installing-by-updating-and-changing-the-current-python-environment","title":"Installing by updating and changing the current Python environment","text":""},{"location":"#individually-package-by-package","title":"Individually Package-by-package","text":"<p>On the command line (or the AnacondaPrompt) in your pc, run these code lines (one at a time):</p> <pre><code>pip install metabolinks\npip install UpSetPlot\nconda install -c conda-forge py-xgboost\nconda install -c pyviz panel\nconda install -c anaconda param\nconda install -c conda-forge holoviews\nconda install -c plotly plotly\npip install python-docx==1.1.0\npip install kaleido==0.1.0post1 # Change version name to 0.1.0 or 0.2.0 if not on Windows\npip install pyopenms\n</code></pre> <p>Tip</p> <p>If you cannot use <code>conda</code> on your command line, use AnacondaPrompt.</p> <p>Info</p> <p>If, after starting any of the versions, any package is still missing install it following the appropriate directions.</p>"},{"location":"#using-requirementstxt","title":"Using requirements.txt","text":"<p>On the command line (or the AnacondaPrompt) in your pc, go inside the directory where you have installed NEMeSIS or open the command line directly in this directory. Then run the following line:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>This will install using <code>pip</code> all the packages used in the software (including some already installed with Anaconda).</p>"},{"location":"#databases-for-annotation","title":"Databases for Annotation","text":"<p>The Git-Hub repository includes currently 3 databases (as <code>.csv</code> or <code>.xlsx</code> files) last updated in February 2023 that you may use for annotation in the software. These are from the Human Metabolome Database (HMDB, https://hmdb.ca/) for human related metabolites, the LOTUS Database (https://lotus.naturalproducts.net/) for natural products and Drug Bank (https://go.drugbank.com/) for drug and drug targets.</p>"},{"location":"GUI_docs/","title":"Get started with GUI","text":""},{"location":"GUI_docs/#starting-the-graphical-interface-of-nemesis","title":"Starting the Graphical Interface of NEMeSIS","text":"<p>NEMeSIS perform the analysis from spectral raw data, m/z peak lists or data tables to statistical analysis where most of this tutorial is focused.</p>"},{"location":"GUI_docs/#opening-nemesis","title":"Opening NEMeSIS","text":"<p>To open NEMeSIS, there are multiple options. We recommend the following:</p> <p>1) Open the Terminal or AnacondaPrompt (if Anaconda was not added to the PATH during installation).</p> <p>2) Navigate to the NEMeSIS folder in the pc with the <code>cd</code> command (or open the terminal directly on the folder).</p> <p>3) If you installed the requirements for NEMeSIS in the NEMeSIS_env specific environment as suggested, activate said environment with <code>conda activate NEMeSIS_env</code>.</p> <p>4) Run <code>Interface_NEMeSIS.py</code> as a line code. This will open the program itself so it can be used in an internet browser (it is running locally however).</p> <p>5) To close the program, return to the command line and press 'Ctrl+C'.</p>"},{"location":"GUI_docs/#introduction-to-the-graphical-interface-of-nemesis","title":"Introduction to the Graphical Interface of NEMeSIS","text":"<p>When opening NEMeSIS, you are greeted with a page such as the one presented below that should open in your browser of choice.</p> <p>Here, you can see on the on the left-hand side of the page (in a black box), you get an index-like column with buttons for every main page of the software. Most will be locked off in the beginning since they require other steps to be completed first before accessing. This is the main way to access different pages in the software. The first button (<code>Home</code>) and initial homepage of the software leads you to an introduction of what this software is and what analysis steps it includes. The second button (<code>Considerations and Instructions</code>) takes you to a few explanations and clarification on the workings of the software, which are also present here at the end of the page in Tips and Precautions. From then on, we go onto the data analysis pages.</p> <p>The green box marks the main section where the page currently selected appears. The red box in the upper right corner has a small ring in it. This ring will indicate to you if the program is currently computing anything. If it is, there will be a small black section going around the ring. While the program is computing, you cannot update anything else in it.</p> <p></p> <p>NEMeSIS is divided into 3 stages:</p> <ol> <li>Data Input</li> <li>Data Pre-Processing and Pre-Treatment</li> <li>Data Analysis and Biological Intepretation (Statistical Analysis)</li> </ol> <p>Note: Independent Side Modules are only available in jupyter notebooks and can be used for further analysis with exported information from NEMeSIS. See Independent Side Modules in the jupyter notebook page.</p> <p></p>"},{"location":"GUI_docs/#stage-1-data-input","title":"Stage 1: Data Input","text":"<p>Three possible inputs are accepted in NEMeSIS: mzML spectral raw data, m/z peak lists of each sample or 2D data tables representing aligned samples. The Data Input page allows to select if your data requires data alignment, that is, if it is one of the the first two inputs mentioned by going to the Data Alignment Page or if the data is aligned and can go to the data table pre-processing performed before data analysis by going to the Data Reading Page. The section of NEMeSIS that allows to perform data alignment is an independent module of all other remaining analysis that will convert your input into a 2D data table. It will auto generate a report since it will not be a part of the report encompassing the remaining steps of the software.</p>"},{"location":"GUI_docs/#data-alignment-page","title":"Data Alignment Page","text":"<p>This page is used to transform either mass lists or mzML data into 2D aligned sample tables that can be used as input for the Data Reading NEMeSIS software. This is geared to the pre-processing of direct-infusion mass spectrometry data. As such, this software has two main purposes:</p> <p>1) Convert mzML file data into mass values and intensity lists - Converting Spectral mzML raw data.</p> <p>2) Perform Data Alignment on mass values and intensity lists to generate 2D aligned sample tables, that can be exported and used as inputs for NEMeSIS - Data Alignment.</p> <p>Moreover, in case the original data was mzML, it also allows to visualize the raw and aligned spectra for the researcher to observe the quality of the alignment - Spectra Visualization.</p>"},{"location":"GUI_docs/#inputting-data","title":"Inputting Data","text":"<p>Input files for the alignment should be placed in the <code>Files_To_Align</code> folder within the NEMeSIS folder. These files will be shown and selectable at the beginning of the software. You can select files either to select to go to Converting Spectral mzML raw data or to pass directly to Data Alignment when you already have your mass lists. The program will recognize this by whether you select one or multiple files:</p> <ul> <li>For submitting mass lists with intensity values to be aligned, the software will either expect a single Excel (<code>.xlsx</code> or <code>.xls</code>) file or multiple <code>.csv</code> files. An example image of the formatting of the Excel file is shown below. It should have one sample per Excel sheet; the name of the Excel sheet should correspond to the sample name (orange box); each sheet should have in its first column the mass values (m/z, neutral mass or equivalent) and in its second column the corresponding intensity values; and finally, the first row should have the name of the two columns, for example, 'm/z' (this name should be consistent between samples if possible) and 'I'. The example file <code>example_samples_to_align.xlsx</code> is available in the <code>Files_To_Align</code> folder as guidance. For the multiple <code>.csv</code> files, the expected formatting for each one is similar to one of the Excel sheets but the name of the column regarding the intensities should be the name of the sample being read. If one file is passed and it is not Excel or multiple files are passed but not all are .csv files, the data will not be read as mass lists with intensity values to be aligned.</li> </ul> <p></p> <ul> <li>For submitting spectral mzML raw data to convert and align, the software will expect multiple mzML files. For the software to assume that mzML conversion is required, multiple files must be passed all in mzML format. Converting spectral raw data into the open mzML format can be done using freely available tools such as Proteowizard's MSConvert.</li> </ul>"},{"location":"GUI_docs/#converting-spectral-mzml-raw-data","title":"Converting Spectral mzML raw data","text":"<p>This section of the software only appears if mzML files are detected in the data input. Here, these files in open mzML format can be converted into m/z peak lists so data alignment can be performed. For this purpose, the pyopenms Python package is used. A series of parameters that can be changed to optimize conversion are available to be chosen with special attention to the <code>Minimal signal-to-noise ratio for a peak to be picked</code> parameter as crucial. Explanations of each parameter are shown with extensive documentation is present in https://openms.de/current_doxygen/html/classOpenMS_1_1PeakPickerHiRes.html. The description from the link is also near every parameter with a tooltip. Credit to pyopenms devs.</p> <p>When pressing the button for conversion, each spectra will be converted giving a small description of the number of raw and centroided signals. After finishing, you will be prompted to download the results, which is required for data alignment. This exports two files, a word document with a small description of the conversion and an excel file with the mass peak lists in the formatting already required for data alignment.</p>"},{"location":"GUI_docs/#data-alignment","title":"Data Alignment","text":"<p>This section of the software either appears after a single Excel fileor multiple <code>.csv</code> files is detected in the input or after mzML spectra raw data conversion and peak list saving. The data alignment performed is made using the <code>align</code> function from the Metabolinks Python package (developed in the group). It uses 2 parameters: the <code>PPM Deviation Tolerance</code> that defines the maximum tolerance (in parts per million) to group metabolic features from different samples together and the <code>Minimum Sample Number Appearance</code> that defines the minimum number of samples a metabolic feature must appear in to be kept in the final aligned dataset.</p> <p>After alignment, a short description of the alignment process will be shown as well as the aligned dataset (if this aligned dataset is large, only the first 10 000 metabolic features will be shown). The dataset and an abridged description of the metabolic feature alignments can be saved. The aligned dataset can be directly used in the NEMeSIS software.</p> <p>Warning</p> <p>The button to go towards the Data Reading Page is only pressable after saving the aligned data. This saved data will be placed in the NEMeSIS folder and can be immediately used as input in the Data Reading Page.</p>"},{"location":"GUI_docs/#spectra-visualization","title":"Spectra Visualization","text":"<p>This section of the software appears after data alignment is performed if the original files were mzML raw data. The objective is to visualize the quality of the spectral data processing by observing the raw, centroided (after spectral processing) and aligned (after data alignment) spectra in interactive graphs of a few samples simultaneously. They are interactive so close ups can be selected for each graph. This allows to observe the genral shape of peaks that were kept through the process and which were discarded and to see if the quality of the aligned spectra is as desired or expected.</p> <p>The image below shows an example. You can choose which samples to see, if the spectra values should be normalized (1), and which mass ranges to plot initially.</p> <ol> <li>Normalization in the raw, centroided and aligned spectra is made by dividing their intensities the sum of all intensities in the raw data specifically.</li> </ol> <p>Warning</p> <p>Spectra contain a very large number of points which combined to their interactive nature, make it heavy resource demanding for the software and the computer. Hence, although it depends on the computer available, we do not recommend selecting more than 3-4 samples simultaneously. Moreover, it may take a few seconds until the spectra are fully operational.</p> <p></p>"},{"location":"GUI_docs/#data-reading-page","title":"Data Reading Page","text":"<p>This page can be reached from the Data Input Page page or the Data Alignment Page. This page reads 2D Data tables (aligned datasets) to be used for the remainder of the workflow. The inputted data requires a strict formatting. This formatting is exemplified in the figure below with the addition that files can either be <code>.csv</code> or <code>.xlsx</code> files. Metabolic Features can either be represented in the rows or columns, while samples and metadata should be on the opposite axis. </p> <p>The example shown is using metabolic features in the rows and samples/metadata in the columns. The format for the opposite case is the exact same but transposed. The first column in your data must correspond to the identifier of your metabolic features, more specifically, to a mass value if possible - black box - and they should be unique (non-repeating). The name of the column will be overwritten to be <code>Metabolite Label</code>. If the columns are mass values capable of being intepreted as floats (numbers), then a <code>Mass</code> column in your data will be added. This column is later necessary to perform <code>Data Annotation</code> (but nothing else). You have 3 possibilities to choose from: <code>Neutral</code> where the masses in your index are taken as the neutral masses creating a <code>Neutral Mass</code> column, <code>m/z (Positive)</code> or <code>m/z (Negative)</code> where the masses are assumed to be m/z values obtained in positive or negative (respectively) ionization mode creating a <code>Probable m/z</code> column.</p> <p>Danger</p> <p>Avoid having a column named <code>Neutral Mass</code> or <code>Probable m/z</code> in your data, since it will be overwritten.</p> <p>The remaining columns are free to be any metadata you have in your data (green box) and the samples with the corresponding intensity values (purple box). If you are using LC-MS or GC-MS or any other kind of hyphenated method and have another column other than mass to characterize your features such as retention time, you can still analyse them and identify this columns as metadata but they will not be used for data annotation or any other step in the analysis.</p> <p>Optionally, the first row (after the column names) of your data may include the class labels of each of your samples (orange box) that will be used to automatically generate the target (class labels) later on.</p> <p>After data is read, the next step of the analysis will be unlocked (loading an example dataset to test the software is possible).</p> <p></p> <p>Info</p> <p>The file <code>5yeasts_notnorm.csv</code> is the default example in the software that can be loaded by NEMeSIS and that can be used as an example of the formatting.</p> <p>Note</p> <p>If you want, you can load the parameters used in a previous analysis at this stage as well. Further details about this in Report Generation and Parameter Saving.</p>"},{"location":"GUI_docs/#stage-2-data-pre-processing-and-pre-treatment","title":"Stage 2: Data Pre-Processing and Pre-Treatment","text":"<p>The second stage of the software is the Data Pre-Processing and Pre-Treatment that includes many sub-sections: Metadata Selection, Data Filtering, Data Annotation, Formula Assignment, De-Duplication of Annotated Features and Data Pre-Treatment.</p> <p>All these sections are mandatory and are progressed and unlocked one by one in a linear fashion. You can redo previous parts of the pre-treatment. If you change any part of the pre-treatment, posterior pre-treatment steps and statistical analysis will immediately be locked and will erase previous statistical analysis if any has been performed. This is to avoid situations where the user wanted to change the pre-treatment but forgot to re-apply posterior pre-treatment steps. Thus, it is necessary to keep coherency.</p>"},{"location":"GUI_docs/#metadata-selection","title":"Metadata Selection","text":"<p>Here, you will be asked to select which columns of your data are metadata (non-sample columns). If this metadata includes formula assignment or metabolite compound annotation columns, you can select them in their corresponding spots so they can be used downstream as that. Furthermore, you can select a column that contains the mass values of your metabolic features, whether it is the <code>Neutral Mass</code> or <code>Probable m/z</code> columns created in the previous section or another prepared previously that will be used for <code>Data Annotation</code> (if <code>None</code> is chosen, <code>Data Annotation</code> cannot be performed). Finally, you can select Quality Control (QC) samples that will be separated from the remaining ones (they will not be used in the statistical analysis but can be used for Quality Control Sample based Data Filtering). Other metadata will not be used in the analysis.</p> <p>After selecting it, either a target will try to be inferred from the name of your samples or it will be taken from your data (if it was present). In either case, you can edit your target to your needs, before confirming it and moving to the next section.</p> <p></p>"},{"location":"GUI_docs/#data-filtering","title":"Data Filtering","text":"<p>Data Filtering is used to clean the dataset of features that appear in very few samples. You can select to perform Data Filtering by 4 different procedures.</p> <p>1) Filtering based on the number of samples each feature appears in. This will only include features that appear in a determined number n of either the total samples of the dataset (<code>Total Samples</code> method) or of at least the samples of one class (<code>Class Samples</code> method). It can be used, for example to filter experimental artifacts. This n is referred to as the feature filter keyword and can go from 1 to the number of samples in your data in the first case or to the number of samples of your least populated class in the second.</p> <p>2) Filtering based on intensity values. This will filter metabolic features based on their intensity values as calculated by the mean or median over the samples they appear in, removing lower intensity features. A hard threshold with a set intensity value is used or a percentage based threshold where the percent lowest intensity of features are removed.</p> <p>3) Filtering based on the feature variance of Quality Control samples. Only available if you have at least three QC samples as selected in the previous page. This will eliminate features that have high variance in the quality control samples (where they should be equal) as estimated by relative standard deviation (standard deviation / mean) above a determined threshold, since that points that they are not reproducible. Features that do not appear in the QC samples are assumed to have 0 variance.</p> <p>4) Filtering based on the feature variance of analysed samples. This filtering is not recommended to use if you are looking for exclusive metabolic features in the classes of your datasets or if you aim to use feature occurrence data. It will filter out the lowest variance features across the samples since their intensity patterns would not be informative.</p> <p>After Data Filtering, a small table detailing some characteristics of your data as well as your dataset will be shown.</p>"},{"location":"GUI_docs/#data-annotation","title":"Data Annotation","text":"<p>Next up, we have Data Annotation. This step is only possible if you have a Mass column as selected in the earlier section (either created in the software based on mass values provided or previously obtained) that represents the masses of the metabolites detected as numbers. This section allows to match unknown metabolic features to known compounds and metabolite, highly enhancing the possible biological interpretation.</p> <p>You can skip this step by selecting annotation with 0 databases. Or you can select 1 to 5 databases to perform independent annotations. A database must have a compound ID column, a compound name column and a compound formula column (that is used to calculate the compound theoretical mass used for annotation). To use a database, it should be on the directory where NEMeSIS is located and you must provide a series of informations such as the file name and the column identifier of the ID, name and formula of the compounds. You will also be asked for an abbreviated name of the database to use (for example, HMDB for Human Metabolome Database). If the Database contains a column with KEGG compound IDs, this information will also be conserved in the annotation if and only if the column is named 'kegg'.</p> <p>After loading your desired databases, you can choose a maximum threshold of deviation between the theoretical masses of the compounds in the database and the neutral masses in the dataset. This can be a flat threshold (<code>Absolute Dalton Deviation</code>) or a parts per million based threshold (<code>PPM Deviation</code>) (1).</p> <ol> <li>A usual value for extreme-resolution data could be a ppm deviation between 0.5 and 1 ppm.</li> </ol> <p>Finally, you can decide which adducts to search by inputting the adduct name and consequent mass shift they cause on a neutral mass. This should be inputted in the following format \"Adduct_Name : Mass_Shift_to_neutral_mass\". As an example: \"[M+H]+ : 1.007276451988935\". The software automatically provides an example of common adducts to search based on if <code>Positive (m/z)</code> ([M+H]+, [M+Na]+, [M+K]+), <code>Negative (m/z)</code> ([M-H]-, [M+Cl]-) or <code>Neutral</code> ([M]) were chosen in Step 1.</p> <p>With these parameters selected, you can perform the annotation. Before the annotation, all selected databases will be merged together. The annotation is made by comparing the dataset's masses with the databases. Every compound in the database that fall within the error margin threshold chosen to a given metabolite feature is annotated to that feature. Thus, very often features have multiple annotations associated (for example, every isomer in a database will be annotated if one is). This annotation is based on the fact that the mass value of a feature is not enough to differentiate and choose between different isomers. The results of the annotation of our data based on an individual database is the creation of 5 metadata columns: for each metabolic feature, one has a list of compound IDs annotated, another of compound names, another of compound formulas, another with the corresponding adducts and the last with the database where the annotated compounds were present (1). If KEGG compound IDs are detected in a database a 6th metadata column will be created with these KEGG IDs (2).</p> <ol> <li>The lists are in the same order in the 5 columns. Thus, the 3rd compound ID in its list corresponds to the 3rd compound name in its list.</li> <li>The KEGG ID columns are detected by their name - the name must be 'kegg'.</li> </ol>"},{"location":"GUI_docs/#formula-assignment","title":"Formula Assignment","text":"<p>Formula Assignment is a complement to the Data Annotation performed in the step before. Data annotation provides a lot of information to annotated metabolic features but the lower number of compounds in the database leads to only a small subset of metaoblic features to be annotated. Formula assignment complements annotation by having a much wider coverage of the chemical space allowing many more formula assignments to be performed, however each is less informative individually. Thus, they complement each other to provide a fuller picture of the dataset.</p> <p>Tip</p> <p>Data Annotation and Formula Assignment are essential for the extraction of biological information. However, if annotation and/or formula assignment have been performed outside NEMeSIS, we do not recommend them to also be applied here. Multiple sets of annotations or of formula assignments can confound the De-Duplication of Annotated Features step (next step) and posterior analysis.</p> <p>Formula assignment is made here with our developed algorithm ScoreForm presented with detailed descriptions of the criteria and steps used available in the preprint here. It follows the 7 Golden Rules proposed by Kind and Fiehn updated to new standards following what is observed in the chemical space of metabolites nowadays. It focuses on restricting and prioritizing formulas with elemental ratios (and number of atoms of each element) in ranges tailored to the mass of the formula itself, allowing a higher performance in formula prediction while not sacrificing formula coverage. Moreover, it is a score-based algorithm providing a relative score of confidence when there are multiple candidates for assignment of a metabolic feature. The scores and candidates are also saved within the dataset allowing the researcher to delve into the candidates and compare their relative scores for greater granularity.</p> <p>Importantly, the first step of the algorithm is the creation of a Formula Database which is performed outside the interface. Without creating a database, this step cannot be ran and must be skipped. The Formula Database can be created by simply running the <code>FormulaDatabaseCreator.ipynb</code> notebook once before. The parameters in this file can also be changed to suit the Formula Database closer to the researcher's desires. The name of the files comprising the database should follow the convention of 'formulas_improved_dict{n}.csv' where n is the lowest molecular mass in the file that should go from 0 to 1000 in 250 Da intervals. For example, 'formulas_improved_dict250.csv' should comprise the masses from 250-500 Da.</p> <p>Parameters shared with data annotation, namely mass deviation and adducts searched must be kept the same as for data annotation so as to not introduce incompatibilities. Thus, to change these parameters, they have to be altered in the Data Annotation page. After formula assignment, a small description of the results is shown.</p> <p>Warning</p> <p>When choosing the name of the columns the assigned formulas will be added to, make sure it is not the same name of a pre-existing column.</p> <p>Note</p> <p>The next version will include a greater set of parameter options to modify and finetune how the ScoreForm algorithm is applied.</p>"},{"location":"GUI_docs/#de-duplication-of-ann-features","title":"De-Duplication of Ann. Features","text":"<p>Due to the proximity of mass values, the same compound (or formula) can be annotated to different metabolic features. The objective of this section is to merge metabolic features that have the same annotations or formula assignments, preventing the existence of features with the same annotations. The rationale is that, being NEMeSIS targeted to direct infusion data, the adduct of one compound should result in one monoisotopic mass and therefore metabolic peak. However, practically, an m/z peak might be split between two very similar m/z values. Thus, we can merge features with the same annotations. In the same way, peaks from different adducts can also be merged into one metabolic feature.</p> <p>The beginning of the page will provide you a report of duplicated peaks across annotations and formula assignments you have provided (see figure below). The first columns regard to the duplications in annotations with the orange box showing the duplications from columns previously annotated before uploading data to NEMeSIS and in green the box with Matched IDs annotations performed by NEMeSIS; and the last columns correspond to duplications in formula assignments with the first (red box) corresponding to the NEMeSIS performed ScoreForm assignment and the last one (blue box) to previous formula assignments. This example shows all 4 types of possible columns to appear but we recommend for no more than 1 annotation and 1 formula assignment column to be present in the data to simplify de-duplication and analysis.</p> <p></p> <p>From this report, a decision can be made whether de-duplication (merging) of these peak will be made. The most common situations to merge is the existence of one 'main' peak with much higher intensities across the samples or the existence of peaks corresponding to multiple adducts that can be merged. More situations can also arise, including a specific situation that warrants individual attention. These are more likely when multiple annotations are considered, hence our previous recommendations. When finding different metabolic features with the same annotation (for all columns), features are merged by keeping the highest intensity value in each sample if representing the same adduct and summed if they correspond to different adducts (1).</p> <ol> <li>When both cases happen at the same time, first metabolic features of the same adduct are merged by keeping the highest intensity and then different adducts are summed.</li> </ol> <p>In total, 4 possible situations for merging can happen:</p> <ul> <li> <p>Situation 1 (Overwrite): Multiple metabolic features have the same annotation for all annotation databases with the highest intensity values coming all from one metabolic feature that becomes the de facto metabolic feature with others being erased (only happens when they all come from the same adduct).</p> </li> <li> <p>Situation 2 (Merge Same Adducts): Multiple metabolic features have the same annotation (no different annotations in any annotation column) with the highest intensity coming from at least two different metabolic features and ALL metabolic features come from the same adduct. For each sample, the highest intensity is kept. 'Bucket Label' and the 'Mass' columns become the weighted average (based on the average intensity of the features) of all the features with the same annotation.</p> </li> <li> <p>Situation 3 (Merge Different Adducts): Identical to Situation 2 but there is at least one metabolic feature that comes from a different adduct. For each sample, the highest intensity of metabolic features from the same adduct are kept. Then, the intensities of metabolic features of different adducts are summed.</p> </li> <li> <p>Situation 4 (Contradictions - Possible Problem Situation): Multiple metabolic features have the same annotation for one annotation and different for another. Most of the time there is no issue. For example, imagine a case where we have annotated HMDB compounds and ScoreForm assigned formulas. Scenario 1: We find that HMDB puts two different compounds for 2 metabolic features and ScoreForm assigns the same formula. Here, it is fair to treat them as different features and this is the default behaviour. Scenario 2: However, rarely, there can be a case where there are more than 2 metabolic features with the same compound annotated in HMDB. This should be quite rare and it is from where the problem arises. HMDB has the same compound for 4 features and ScoreForm assigns to one of them one formula, to a second one a different formula and the last two no formula at all. What is the correct course of action?  Perhaps merging the two peaks with no annotation by ScoreForm, maybe merging those two peaks with one of those annotated by ScoreForm since they would be normally merged if not for the existence of two different ScoreForm annotations. Hence, the problem. They should be seen on a case-by-case basis and are thus not automatically merged. These cases can be individually seen and decided upon (example below). At this point, we believe a sensible course of action may be to not merge any of these peaks together. If many of these cases occur, consider your annotations.</p> </li> </ul> <p>From the merging of metabolic peaks, tables will appear to describe them as shown below. First, an overview of the mergings performed on the left and the mergings that each annotation led to (by order of scanning) on the right. Below, a description of every single merging is also provided.</p> <p></p> <p>Info</p> <p>It is expected for the number of mergings per annotation to decrease along annotations since most of the annotation duplications and mergings overlap. In our example, almost all mergings that would happen for the <code>Matched IDs</code> column happenned first for the <code>Name - Prev. Ann.</code> column and, therefore, did not need to be repeated.</p> <p>At the end, all possible merging problems arising from Situation 4 will also be displayed (figure below), where you can pass to the Data Pre-Treatment or decide what to do with each individual problem. Right now, we believe a sensible course of action may be to not merge any of these peaks together and consider your annotation more closely if many of these cases occur.</p> <p></p> <p>The figure below shows the example of a problem emerging from a 5-peak merging (5 peaks being merged is highly unlikely and only occurs due to the wide ppm deviation allowed for a prvious annotation made in the example dataset). Here you can select on the elft which peaks to merge. However, these peaks cannot be incompatible. For example, in the case shown below, despite all peaks having the same annotation in the <code>Name</code> and <code>Formula</code> columns (previous annotation) and no annotation in <code>Matched IDs</code>, they all have different annotations in the <code>Formula_Assignment</code> column. Therefore, no possible selection will allow a merging of the peaks. If for example, another peak also had no annotation in the <code>Formula_Assignment</code> column like <code>306.0777699814 Da</code>, then those two peaks could've been selected and merge (identical for if any 2 peaks had the same <code>Formula_Assignment</code> assignment). After this, a report of the individual mergings performed or not performed will be provided before moving on to the next section.</p> <p></p>"},{"location":"GUI_docs/#data-pre-treatment","title":"Data Pre-Treatment","text":"<p>Data Pre-Treatment is a series of successive operations that include: missing value imputation, normalizations, transformations and scaling. The only mandatory operation is missing value imputation since the existence of missing values does not allow the application of many statistical methods downstream. These procedures have the aim of highlighting relevant biological variation while reducing the effect of undesired variation (van den Berg et al., 2006) and make the values in dataset more amenable to posterior downstream analysis.</p> <p>Each category has multiple different approaches available which we will present next and, since data pre-treatment highly impacts posterior data analysis, careful deliberation on the combination of pre-treatments to use is advised.</p> <p>Danger</p> <p>Not all possible combinations of pre-treatments are viable. There are some combinations which included incompatible methods between them or with your data. As two examples, Zero missing value imputation cannot be used with a generalized logarithmic transformation and normalization by a reference feature cannot be used if your reference feature does not appear in all the dataset samples. This incompatibility will generate missing values in the data which the software will warn you about.</p> <p>Missing Value Imputation</p> <p>Missing Value Imputation aims to replace missing values in the dataset (metabolic features that do not appear in certain samples despite appearing in others) with \"probable\" intensity values to make possible many different statistical analysis. There are 3 types of missing values: Missed At Random (MAR), Missed Not At Random (MNAR) or Missed Completely At Random (MCAR). The methods available in this software assume the missing values are MNAR, that is, that they were missed not due to instrumental error but due to the metabolic feature being absent or in concentrations / intensities below the detection limit in that sample. Thus, these methods replace this missing values with low values below most other intensity values in the data.</p> Missing Value Imputation Methods Available Description Minimum of Sample Replace missing values with a fraction (0 to 1) of the minimum intensity value in the corresponding sample. Minimum of Feature Replace missing values with a fraction (0 to 1) of the minimum intensity value in the corresponding metabolic feature. Minimum of Data Replace missing values with a fraction (0 to 1) of the minimum intensity value in the dataset. Zero Imputation Replace missing values with 0. <p>Normalization</p> Normalization Methods Available Description By Total Sum of Intensities Dividing each intensity by the total sum of intensities of the corresponding sample (all values between 0 and 1). By a Reference Feature Dividing each intensity by the intensity of the reference feature in the corresponding sample. Probabilistic Quotient Normalization (PQN) Normalization method proposed by Dieterle et al. in 2006 - see paper here for details. Quantile Normalization As explained by Bolstad et al. in 2003 - see paper here for details. None No Normalization Performed. <p>Tip</p> <p>Besides the incompatibilities mentioned earlier, we recommend not to use Quantile Normalization when your data has a high percentage of missing values, which is more likely with direct infusion extreme-resolution data that this software aims to analyse.</p> <p>Transformation</p> Transformation Methods Available Description Generalized Logarithmic Transformation (glog) Apply the equation: log<sub>2</sub>(y + (y<sup>2</sup> + lambda<sup>2</sup>) /2). When lambda = 0, it equivalent to a logarithmic transformation None No Transformation Performed. <p>Scaling</p> Scaling Methods Available Description Mean Centering \\(\\widetilde{x}_{ij} = x_{ij} - \\overline{x}_{i}\\) Pareto Scaling \\(\\widetilde{x}_{ij} = \\dfrac{x_{ij} - \\overline{x}_{i}}{\\sqrt{s_{i}}}\\) Auto Scaling \\(\\widetilde{x}_{ij} = \\dfrac{x_{ij} - \\overline{x}_{i}}{s_{i}}\\) Range Scaling \\(\\widetilde{x}_{ij} = \\dfrac{x_{ij} - \\overline{x}_{i}}{x_{i_{max}} - x_{i_{min}}}\\) Vast Scaling \\(\\widetilde{x}_{ij} = \\dfrac{x_{ij} - \\overline{x}_{i}}{s_{i}}\\) . \\(\\dfrac{\\overline{x}_{i}}{s_{i}}\\)  (associated paper) Level Scaling \\(\\widetilde{x}_{ij} = \\dfrac{x_{ij} - \\overline{x}_{i}}{\\overline{x}_{i}}\\) (the scaling factor can be the median of \\({x}_{i}\\) as an alternative) None No Scaling Performed. <p>Info</p> <p>Scaling equations obtained from van den Berg et al., 2006, paper available here.</p> <p>After Data Pre-Treatment, you will have the option to download the generated tables in different stages:</p> <ul> <li>Untreated data after data annotation - <code>annotated_df.csv</code></li> <li>Treated intensity data without metadata - <code>treated_df.csv</code>.</li> <li>Treated intensity data with metadata - <code>complete_treated_df.csv</code>.</li> </ul> <p>Furthermore, a series of extra files starting with Export will also be generated. These files can then be moved to the NEMeSIS folder and used as input for the Independent Side Modules. These are:</p> <ul> <li><code>Export_TreatedData.xlsx</code> - Excel with 4 sheets, one containing the normalized data (without missing value imputation, transformation or scaling) with metadata, another the treated intensity data without metadata, another with the dataset treated with Binary Simplification (or Spectral Digitalization) method and the last with the treated values after missing value imputation and normalization (but before transformation and scaling).</li> <li><code>Export_Target.txt</code> - File containing the class labels (target) of the dataset's samples in the same order as they appear in the dataset.</li> <li><code>Export_TreatedData.pickle</code> - Treated intensity data without metadata in pickle format to guarantee the 'Bucket Labels' suffer no changes (roundings).</li> <li><code>Export_ProcData.pickle</code> - Normalized data (without missing value imputation, transformation or scaling) with metadata in pickle format to guarantee the 'Bucket Labels' suffer no changes (roundings).</li> </ul> <p>After moving from this page, you will be able to select a colour to represent each of the classes in the studied dataset before moving on to the Data Analysis and Interpretation (Statistical Analysis) step. Futhermore, at this stage, the parameters used for pre-processing and pre-treatment can be saved as a json that can be loaded back in the Data Reading page in subsequent analysis. This will be automatically saved on the NEMeSIS folder.</p>"},{"location":"GUI_docs/#stage-3-data-analysis-and-interpretation","title":"Stage 3: Data Analysis and Interpretation","text":"<p>The third and final stage of the software is the Data Analysis and Interpretation (Statistical Analysis) that also includes many sub-sections: Common and Exclusive Compound Analysis, Unsupervised Analysis, Supervised Analysis, Univariate Analysis, Data Diversity Visualization, HMDB based Pathway Analysis, BinSim Treated Data Analysis and a Compound Search Tool.</p> <p>Almost all these sections are independent of one another and can be performed in any desired order based on the analyses objective. The exception is certain parts of the HMDB based Pathway Analysis section that requires either Supervised Analysis or Univariate Analysis to have been performed first.</p> <p>These steps have different purposes to interpret the analysed data, giving different perspectives on it.</p> <p>Info</p> <p>During this stage, analysis results expressed as images can either be static (made using matplotlib python package)or interactive (made using plotly). For static figures, a nearby button with 'save as png' text will always be nearby so the figure can be downloaded exactly as it is shown. For interactive figures (see image below), when hovering over them, a series of options appears in the top right. The leftmost of these options allows to save the figure exactly as it is currently shown (red box in image below). Interactive figures allows to hover over individual points and obtain information, to zoom in on specific regions, etc. To return the default view point, the autoscale button can be pressed (green box in image below).</p> <p></p>"},{"location":"GUI_docs/#com-and-exc-compound-analysis","title":"Com. and Exc. Compound Analysis","text":"<p>The Common and Exclusive Compound Analysis when computed provides three types of outputs that can be viewed by selecting the shown tabs. They allow to see possible important compounds that are biomarker like as well as the the general number of metabolic features by class and common to all classes. It also allows to see if a subset of classes has a high number of metabolic features specific to them.</p> <p>The Overview section includes a brief description of the number of compounds (and annotated compounds) that each biological class has, that are exclusive to each of them and that are common to all of them. This data can also be downloaded as a table. Furthermore, you may select a subset of the classes to see all associated metabolic features to those classes or only those that are exclusive to those classes. This data subset can also be saved as a dataframe.</p> <p>The Venn Diagram section, as the name entails, shown a Venn Diagram of the metabolic features. This Diagram is built with the <code>venn.py</code> file originally provided in https://github.com/tctianchi/pyvenn/blob/master/venn.py and used according to its license.</p> <p>The Intersection Plot section shows intersection plots considering all metabolic features of the data and only the features with annotated data.</p> <p>Venn Diagrams and Intersection Plots are static non-interactive images.</p> <p>Info</p> <p>Venn Diagrams and Intersection Plots will not be computed and drawn if the dataset has more than 6 different classes since the observation of the data becomes less meaningful.</p>"},{"location":"GUI_docs/#unsupervised-analysis","title":"Unsupervised Analysis","text":"<p>Multivariate Unsupervised Analysis methods included in its corresponding page are Principal Component Analysis (PCA) and Hierarchical Clustering Analysis (HCA). Since they are used to observe the intrinsic patterns and structure in the data, they provide mostly figures to describe the analysed data.</p> <p>The PCA section can show 2D and 3D projection of the samples in 2 or 3 PCA components (these components can be chosen) as well as the cumulative explained variance by component and a 2D Scatter plot of PCA projections of up to the first 6 components. All these figures are interactive.</p> <p>The HCA section contains the dendrogram result as a static image that can e downloaded. Changes to the parameters will immediately cause an update to the dendrogram.</p>"},{"location":"GUI_docs/#supervised-analysis","title":"Supervised Analysis","text":"<p>Multivariate Supervised Analysis methods included in its corresponding page are Partial Least Squares - Discriminant Analysis (PLS-DA) and Random Forest (RF). They are used to build classifier models based on stratified cross validation schemes and obtain a list of important metabolites to build these classifiers and discriminate between classes.</p> <p>Both PLS-DA and RF follow a very similar organization on their corresponding tabs.</p> <p>They start with an optimization of specific parameters for each one. For PLS-DA, this is an optimization of the number of components or latent variables which is an essential parameter. This is made by evaluating the \\(Q^{2}\\) (1) and the \\(R^{2}\\) (2) (see tip below) when fitting a PLS model to the data from a chosen initial to final number of components with a stratified cross validation scheme (see warning below). For RF, it is an optimization of the number of trees by observing RF accuracy of models built between specified numbers of trees with a stratified cross validation scheme.</p> <ol> <li> <p>Mean squared error of PLS predictions based on test samples.</p> </li> <li> <p>Mean squared error of PLS predictions based on training samples.</p> </li> </ol> <p>Warning</p> <p>The maximum number of components chosen cannot be higher than the number of samples that will train a model minus 1. For example, if you have 15 samples and a 3-fold cross-validation each fold will have 5 samples. A training set will be comprised of two of those folds thus it will have 10 samples, thus the number of components cannot be higher than 9. Another example if you have 22 samples and 5 folds, the folds will have 4/4/4/5/5 samples each. A training set will have four of these folds and the minimum sum of them is 4+4+4+5-1=16, thus it cannot be higher than 16.</p> <p>Tip</p> <p>When optimizing PLS-DA, the chosen number of components should be the number where \\(Q^{2}\\) stops increasing.</p> <ul> <li>\\(Q^{2}\\) will increase until a certain number of components that should be chosen. Then it usually stabilizes but from a certain point it might start to decrease which would mean the model is overfitting (less generalizable to the test samples).</li> <li>\\(R^{2}\\) will be higher than \\(Q^{2}\\) but it should not be used to choose the number of components. This metric always increases with more components (estimation errors might lead to temporary decreases) which means it will eventually overfit the model.</li> </ul> <p>When optimizing RF, the usual pattern observed is an increase in accuracy with an increase of the number of trees until it stabilizes and fluctuates around a certain value. Since RF are somewhat resistant to overfitting, the value chosen will be one where the accuracy has stopped increasing but not too high as to make the model training too slow.</p> <p>After optimization, the models can be trained allowing the modification of key parameters. The number of times to repeat analysis parameter exists as more repetition will allow a better estimation of model peerformance and feature importance at a cost of time of running. They will output a table summarizing model performance and a table with an ordered list of important metabolic features to build the classifier model as estimated by either Variable Importance in Projection (VIP) scores or X-Weights for PLS-DA and by Gini Importance for RF. The latter table can be downloaded.</p> <p>Info</p> <p>Metabolic Feature importances estimated are calculated for each fold and each repetition of the analysis and averaged.</p> <p>VIP Scores are calculated based on Keiron Teilo O'Shea provided code in https://www.researchgate.net/post/How-can-I-compute-Variable-Importance-in-Projection-VIP-in-Partial-Least-Squares-PLS. This calculation can be slow when you have a high number of features.</p> <p>Besides model fitting, Permutation tests for model validation and Receiver Operating Characteristic (ROC) Curve are available.</p> <p>ROC Curves plot the True Positive Rate by the False Positive Rate and thus geared towards 2-class datasets (1). For multiclass cases (more than 2 classes), a ROC curve will be computed for each class considered as the positive by fitting a \"1vsAll\" model, that is, the class of each sample is either the current positive class or \"Other\". Thus, there will likely be many more negative samples than positive samples in each case, greatly increasing the importance of the latter for the curve of each class.</p> <ol> <li>One class has to be considered as the positive class.</li> </ol> <p>Info</p> <p>Permutation tests are slow to be computed and can take quite a few minutes depending on the number of permutations. For an initial analysis of the data, it may be skipped.</p> <p>Permutation test p-value is calculated as: \\(p-value = \\dfrac{1 + n}{N}\\)</p> <p>Where n is number of times permuted model has better performance that non-permuted model and N is the total number of permutations.</p>"},{"location":"GUI_docs/#univariate-analysis","title":"Univariate Analysis","text":"<p>Univariate Analysis page includes 1v1 Univariate Analysis and Multiclass Univariate Analysis. It allows to detect metabolites that individually have a significant difference between the tested classes and can thus be discriminant. Each metabolite in the dataset is tested individually and thus, this does not account for any interaction between metabolites as multivariate analysis does (and is expected in metabolites within a biological system).</p> <p>1v1 Univariate Analysis includes also a Fold-Change analysis. When there are more than 2 classes, a control and test class can be chosen. In this case, the samples respective to the 2 chosen classes will be selected from the non-treated data (after annotation). Data filtering (Filter 1 only) and pre-treatment will be performed the same way it was done on the complete dataset before performing unsupervised analysis (starting from the dataset after data filtering, annotation and de-duplication) (1). In case of filtering based on the total number of samples a feature appears in, the number used on the full dataset is converted to percentage-based and then a new number is calculated based on the number of samples of the control and test class, which is then rounded up.</p> <ol> <li>We cannot start from the completely full dataset since we have no annotations associated with it despite the fact that, when using total number of samples based data filtering, there could be some fatures that were initially filtered out that could remain in the dataset. E.g. if you choose a minimum of 4 samples to appear in a 15-sample 5-class dataset. When performing univariate analysis on 2 classes that have a total of 6 samples, the minimum 5 samples would be transformed to a minimum of 2 samples in the 6-sample subset (4 out of 15 represents 26.67% of samples, which is 1.6 out of 6 samples rounded up to 2). There could be features that appear in 2 of these 6 samples but that did not appear in 4 of the original 15 samples and were thus previously removed from the dataset. These features stay removed.</li> </ol> <p>The types of univariate test can be chosen (t-test, Mann-Whitney test, consider variances equal or not) as well as the thresholds for significancy on both p-values (1) and fold-change. Fold-change is calculated using the values after missing value imputation and normalization (but before transformation and scaling) as to avoid calculations with negative values. Thus, fold-change calculation is severely affected by a high number of missing values and is not suited to this type of data where the values have to be taken with a grain (or multiple grains that are actually more like rocks than grains) of salt (see tip below).</p> <ol> <li>P-values are adjusted with Benjamini-Hochberg multiple test correction.</li> </ol> <p>Tip</p> <p>Selecting the Fold-change threshold to 1 will nullify it, essentially skipping the threshold. This should be used when the dataset has a high number of missing values.</p> <p>1v1 Univariate analysis results include a downloadable table with all p-values and fold changes, a volcano plot and a clustermap that can show the top most significant features or all significant features below a determined p-value threshold. Finally, for multiclass datasets, you can also obtain the intersection of significant metabolites against multiple possible test classes.</p> <p>Multiclass Univariate Analysis can only be performed when the dataset has more than 2 classes. All classes are tested simultaneously and results show significant metabolites considering all samples not possible for 1v1 analysis. On the other hand, no fold-change can be calculated or volcano plot drawn. The test performed can be chosen between the parametric ANOVA and non-parametric Kruskal-Wallis test.</p> <p>Multiclass  Univariate analysis results include a downloadable table with all p-values and a clustermap that can show the top most significant features or all significant features below a determined p-value threshold. This clustermap can show the groupings of samples and also of metabolic features and their intensity pattern trends.</p>"},{"location":"GUI_docs/#data-diversity-visualization","title":"Data Diversity Visualization","text":"<p>Data Diversity Visualization allows to see the chemical diversity in different ways of your dataset. The section presents three types of plots: Van Krevelen Plots, Kendrick Mass Defect Plots and Chemical Composition Series plots. All these are computed when pressing the button at the top of the page and can be accessed by clicking their respective tab. Each has multiple options that allows to tune the figure. An essential parameter common to all 3 plots is which columns containing formulas to consider whether it is from data annotation, formula assignment or previous annotations and formula assignments performed outside NEMeSIS. Multiple can be selected to consider multiple annotations (each plot deals with multiple annotations in their own way) but at least one column must be selected.</p> <p>Van Krevelen Plots are drawn one for each class considering the metabolites annotated in the samples of each class. If multiple formulas can be assigned to a metabolite whether within the same database annotation or different, they are both considered and plotted.</p> <p>Kendrick Mass Defect Plots are drawn one for each class considering the metabolic features detected in the samples of each class.The colour of each feature is based on its chemical composition series. If multiple formulas can be assigned to a metabolite AND they do not belong to the same chemical composition series, they get assigned as 'Ambiguous'.</p> <p>In Chemical Composition Series, if multiple formulas can be assigned to the same metabolic feature, whether within the same annotation made or between different annotations, each one will be counted. That is, if a feature has 3 possible formulas, 2 belonging to the 'CHO' series and another to the 'CHOP' series; then 2 formulas will be added to the 'CHO' series and 1 to the 'CHOP' series. Thus, we are considering that the 3 elemental formulas are represented by that feature (probably an overestimation). To provide an idea of how extensive this effect is, a description is also provided before the plot detailing how many formulas are being considered for each class and from how many different features they came from.</p> <p>Bug</p> <p>Randomly, in some Kendrick Mass Defect Plots, points may have a bigger size than in other plots despite dot size being parametrized the same in both cases.</p>"},{"location":"GUI_docs/#hmdb-based-pathway-analysis","title":"HMDB based Pathway Analysis","text":"<p>HMDB based Pathway Analysis page can be divided into 4 separate and distinct sections:</p> <ul> <li>Pathway Assignment (HMDB Annotation)</li> <li>KEGG Colour Mapping</li> <li>Pathway Over-Representation Analysis (based on HMDB IDs)</li> <li>Mapping SMPDB Pathways (based on HMDB IDs)</li> </ul> <p>Three of these sections require HMDB annotation while the KEGG Colour Mapping requires that only 2 classes are present in the dataset.</p>"},{"location":"GUI_docs/#pathway-assignment-hmdb-annotation","title":"Pathway Assignment (HMDB Annotation)","text":"<p>This section matches known metabolic pathways to HMDB annotations (1) with 55872 HMDB compounds having at least 1 associated pathway. For matching, a column with HMDB identifiers must exist. The identifiers should have 7 numbers after HMDB (e.g. 'HMDB0000001'). The output gives the HMDB IDs and annotation names as index in a DataFrame and the name and id of the pathways that they belong to.</p> <ol> <li>Only matched to HMDB and the abbreviation of the database should be HMDB.</li> </ol> <p>Moreover, a specific search of the associated pathways of an HMDB identifier in the RAMP database can be performed in the <code>HMDB ID Pathway Searching Section</code>, independent if that compound was annotated in the studied data.</p> <p>Note</p> <p>Matching is performed based on a file created from the RAMP database (https://rampdb.nih.gov/), which are an aggregation of pathways from multiple sources: HMDB, Reactome, WikiPathways and KEGG.</p>"},{"location":"GUI_docs/#kegg-colour-mapping","title":"KEGG Colour Mapping","text":"<p>This section extracts every KEGG compound ID annotated to the dataset to a downloaded file containing a list with only the KEGG identifiers and with a colour associated with them based on metabolite presence in classes. This file can then be used as input in the KEGG website here.</p> <p>To perform this mapping, 2 conditions must be fulfilled:</p> <p>1) Dataset must only have 2 classes (not available for more than 2).</p> <p>2) A <code>Matched KEGGs</code> column might exist. This column can be metadata before previously included in the dataset or can be generated in this software during Data Annotation if at least one of the databases used for annotation has a column with the name <code>kegg</code> (with no capitalization) which will create the Matched KEGGs column.</p>"},{"location":"GUI_docs/#pathway-over-representation-analysis-based-on-hmdb-ids","title":"Pathway Over-Representation Analysis (based on HMDB IDs)","text":"<p>This section uses the pathway mapping in the Pathway Assignment (HMDB Annotation) section to perform an over-representation analysis and observe which pathways are over-represented. Parameters to choose include, the minimum number of metabolites that a pathway must have in the database to be considered and the minimum number of metabolites detected in the studied dataset of a pathway for that pathway to be considered in the analysis; the background set to use; and the metric to select the set of 'significant' metabolites and corresponding threshold.</p> <p>Info</p> <p>Multiple metabolic features with the same annotation will be considered as individual entries. Metabolic features that have multiple annotations that could correspond to multiple metabolites in the same pathway will be counted as a single metabolite. So, analysis of the relevant pathways should be performed.</p> <p>The background set can be chosen between restricting to the number of <code>HMDB annotated metabolites in the dataset with associated pathways</code> or considering <code>All HMDB metabolites in the pathway database</code> (currently not implemented). The background set to choose is critical. If the pathway database is used as background, most metabolic pathways will appear as significant by 'common p-values' even after multiple testing correction and very general pathways with more metabolites such as 'Metabolism' or 'Biochemical pathways: part I' or 'Transport of small molecules' have a good chance of appearing as significant. Thus, pathways with multiple metabolites annotated that constitute a decent part of the pathway should be looked at more carefully. If the <code>HMDB annotated metabolites in the dataset with associated pathways</code> are used as the background, this conservative approach makes it more complicated for any pathway to be considered significant but there is less bias per pathway. In this case, we recommend looking not only at the number of significant and detected metabolites in the pathway but also to the overall number of metabolites in the pathway (in the database), which is included in the output table.</p> <p>There are 4 metrics available to select the significant metabolites (have to have associated at least 1 pathway). In each case, it will only work if the corresponding methodology has been previously ran.</p> <ul> <li>RF Gini Importance - Threshold is the top % of ranks considered significant (e.g. 0.20 for 20%).</li> <li>PLS-DA Feat. Importance - Threshold is the top % of ranks considered significant if below 1 (e.g. 0.20 for 20%) or the threshold for importance if higher or equal to 1 (should only be used for VIP Scores) - Default.</li> <li>1v1 Univariate Analysis - Threshold is the maximum adjusted (for multiple test correction) p-value from the univariate analysis (e.g. 0.05 for adjusted p-values under 0.05). The test class is the class chosen as test during univariate analysis.</li> <li>Multiclass Univariate Analysis - Threshold is the maximum adjusted (for multiple test correction) p-value from the multiclass univariate analysis (e.g. 0.05 for adjusted p-values under 0.05).</li> </ul>"},{"location":"GUI_docs/#mapping-smpdb-pathways-based-on-hmdb-ids","title":"Mapping SMPDB Pathways (based on HMDB IDs)","text":"<p>This section uses the pathway mapping in the Pathway Assignment (HMDB Annotation) section and the SMPDB pathway database to output a representation of a pathway based on the reactions associated with that pathway in SMPDB (1). This mapping is restricted to SMPDB metabolic pathways (2).</p> <ol> <li> <p>SMPDB and the RAMP database are not always identical even when referring to SMPDB pathways. For example, NAD is associated with reactions in Galactose metabolism but it is not included in the RAMP database as a Galactose metabolism metabolite.</p> </li> <li> <p>SMPDB includes many pathways and only around half of them are designated as metabolic. Others such as disease pathways or signalling pathways are not able to be mapped currently.</p> </li> </ol> <p>Note</p> <p>This section will be improved in the next version to include a greater array of pathways.</p>"},{"location":"GUI_docs/#binsim-treated-data-analysis","title":"BinSim Treated Data Analysis","text":"<p>This section and page uses data treated with Binary Simplification (BinSim) or Spectral Digitalization to perform the same analysis as it is performed for the traditionally treated data in the Unsupervised and Supervised Analysis pages. Thus, all instructions and indications are for the use of this method is equal to those pages.</p> <p>BinSim (paper here) was a method developed in our FT-ICR-MS-Lisboa laboratory group. It is an alternative to traditional intensity based pre-treatments that focuses on feature occurrence instead of intensity offering complementary information. BinSim is particularly effective for extreme-resolution data such as FT-ICR-MS data that has a lot of missing values and can more closely detect the set of metabolites present in a sample. Feature occurrence tends to be more reproducible than intensity values. The set of metabolites detected is characteristic of a biological system and can be used for discrimination.</p> <p>Info</p> <p>BinSim consists of transforming all intensity values present in the data matrix to 1 (metabolic features present in the data) and all missing values to 0 (metabolic features absent from the data).</p>"},{"location":"GUI_docs/#graph-based-analysis","title":"Graph-Based Analysis","text":"<p>Info</p> <p>To finish</p>"},{"location":"GUI_docs/#compound-search-tool","title":"Compound Search Tool","text":"<p>With this search tool, you can localize a metabolic feature of interest in your dataset by its index value (<code>Bucket label</code>), its name (by searching all annotation columns either selected as annotation or made during analysis), formula (by searching all formula assignment columns) or neutral mass (by searching the <code>Neutral Mass</code> column if it exists). A series of bar plots and box plots as shown below will then be computed to show the variation of the metabolic feature between samples and classes.</p> <p>Info</p> <p>The values shown are obtained from the original data inputted with only normalization (the one chosen in Data Pre-Treatment being applied). No Missing Value Imputation or posterior parts of the Data Pre-Treatment are applied.</p> <p></p>"},{"location":"GUI_docs/#report-generation-and-parameter-saving","title":"Report Generation and Parameter Saving","text":"<p>The graphical interface offers two different approaches to save your analysis made besides the tables and figures you can download during the analysis. These are the report generation function and the parameter saving and loading function.</p> <p>The report generation creates a folder with a user chosen name. This folder contains a main read-only word .docx file and a myriad of images, interactive figures and tables that support the report. The report includes all of the data reading and data pre-processing and pre-treatment stages and includes the statistical analyses chosen by the user in the corresponding page. When generating the report, a pop-up will appear confirming if there were issues or not in creating the report. The report attempts to describe the analysis made including all the relevant parameters used for the current analysis (1). Furthermore, these parameters are also used for the name of the different figures and tables generated.</p> <ol> <li>If the parameters selected were changed after the analysis is made and the analysis was not redone, then the parameters in the report should still be the ones originally used in the analysis.</li> </ol> <p>Note</p> <p>If you find a mistake in the generated report, such as a parameter used in the analysis not being the one that is in the report or in the associated figures, please inform us through https://github.com/fticr-sms/Metabolomics_DataAnalysis_Pipeline/issues. We would greatly appreciate so we can iron out any loose ends and problems with the software.</p> <p>Parameter saving is a function that specifically aims to save the currently used parameters in the analysis for future use on another analysis. It is available at two different locations in the interface: at the end of the data pre-processing and pre-treatment stages and in the report generation page. The former only saves parameters regarding the data pre-processing and pre-treatment steps while the latter saves all currently used parameters in the software. Although most more significant parameters are saved, there are some that cannot be saved since they are very specific to the dataset currently under analysis. A few examples are the metadata columns selected or the control/test class used in Univariate Analysis.</p> <p>The idea behind parameter saving is that you can load in these parameters and the <code>Data Reading</code> page of the software to use in other posterior analysis. Furthermore, when loading in parameters, there are some parameters that might not be applicable to the current case such as the feature used in Normalization by a Reference Feature or the minimum number of samples used in data filtering. For these cases, a check will be made to see if they can be used and if not, the default value will remain.</p> <p>Warning</p> <p>Some parameters are saved but should still always be adapted based on the dataset analysed. A case and point is that of the number of components used for building PLS-DA models. This number highly affects the performance of the models and is characteristic of the dataset used. Thus, it should always be adapted to the current dataset. Other parameters such as pre-treatment related can be used on a standard preferred workflow independent of the dataset analysed.</p> <p>Info</p> <p>Within the saved parameters, there are some that are saved based on what is currently present in the program and not what was originally used for the analysis: these are the methods used for the common and exclusive compound analysis, the minimum and maximum number of components used for optimization of the PLS-DA, the model performance evaluation metrice used, the dpi of the permutation figure and number of iterations used in the ROC analysis of the PLS-DA section (for both intensity treated data and BinSim treated data) and the model performance evaluation metrics used and the dpi of the permutation figure of the Random Forest section (for both intensity treated data and BinSim treated data).</p>"},{"location":"GUI_docs/#closing-and-resetting-the-software","title":"Closing and Resetting the Software","text":"<p>To close the program, we recommend going to the terminal / command line tab that opened when you opened the software and closing it, which will terminate the program. Closing the software tab in your browser without closing the terminal will keep the program running in the background.</p> <p>If you want to redo your analysis with another dataset, you may use the <code>RESET</code> button available at the end of the index on the left-hand side. This will reset all your current analysis and parameters chosen. A similar effect but with a softer reset happens when you change a parameter in any of the pre-processing or pre-treatment stages, however you will not be able to read another dataset until you perform a full reset of the software.</p> <p>Bug</p> <p>After resetting the software once using the <code>RESET</code> button, when clicking it again, the window pane to confirm the reset might not appear. To fix this issue, refresh the image and it should appear once again.</p>"},{"location":"GUI_docs/#tips-and-precautions","title":"Tips and Precautions","text":"<p>Note</p> <p>This section is nearly identical to the <code>Instructions and Considerations</code> page of the NEMeSIS software.</p> <p>Here, we will go through some general considerations to have while using this software by describing how it works and some of the currently known issues overall that exist. Furthermore, if you encounter an issue or problem with the software feel free to inform us at https://github.com/fticr-sms/Metabolomics_DataAnalysis_Pipeline/issues/new. If you feel some parts are underexplained or cannot be intuitively grasped, please also inform us so we can improve the descriptions provided.</p> <p>General Considerations to Have</p> <ul> <li> <p>While there is an effort for figures and tables produced that can be downloaded to have the relevant parameters used to build them in the name, it is a good practice to keep in mind what parameters were used to make these figures and tables, especially those that do not update with every parameter change to safeguard possible errors that may arise in the filenames generated.</p> </li> <li> <p>All tables and figures are saved in the <code>Downloads</code> folder. Interactive figures are downloaded using the toolbar that appears at the top right of the figure when hovering over it. Non-interactive figures are saved using an available button in the software. The generated report folder is put on the current working directory, that is, the folder where NEMeSIS is.</p> </li> <li> <p>When you start running something there is not a way to stop the process. You can observe if the program is running by observing the little circle on the top right. If there is something currently running it will have this aspect: . Otherwise, it will have this aspect .</p> </li> <li> <p>Resetting the program will reset all your variables and previously performed analysis.</p> </li> </ul> <p>Possible Problems and Suggested Fixes</p> <ul> <li> <p>A rare bug that occurs when running the program is the de-formatting of the pages that can happen when you go back and forth between different pages causing artifacts of different pages or empty spaces before the pages to show up. We currently believe this is an issue with the panel package we are basing this software on. This can be mostly solved by refreshing the page in the browser to reset the layout of the page. However, do not use this trick on the common and exclusive compound page since this has a small chance to make the sidebar to navigate the software disappear, rendering it unoperable and needing to be closed and opened again.</p> </li> <li> <p>Another issue of the same type is the reset floatpanel not appearing when pressing it the 2nd time. It is also fixed by refreshing.</p> </li> <li> <p>Do not mash the buttons many times, it may buffer the clicks. See first on the upper right if the program is running or not.</p> </li> <li> <p>Other known issues have smaller consequences and affect specific pages. When they exist, a little note in bold will indicate the problem at the beginning of the corresponding page.</p> </li> </ul> <p>Performance of the Different Parts of the Program</p> <ul> <li> <p>The program generally runs smoothly and quickly with low latency. The exceptions are the very first page when data reading which can slow down the program for a few seconds as well as some parts when certain analysis such as PLS-DA or Random Forest finish and the page layout is updated with the results.</p> </li> <li> <p>The possible slow downs and time of analysis is also dependent on the size of the dataset. The bigger the dataset to be analyzed, the slower the analysis is and vice-versa. Formula assignment or calculating VIP scores for example is heavily dependent on dataset size.</p> </li> </ul>"},{"location":"jupyter_docs/","title":"Get started with jupyter","text":""},{"location":"jupyter_docs/#introduction-to-the-illustrative-jupyter-notebook-containing-the-workflow-in-nemesis","title":"Introduction to the illustrative jupyter notebook containing the workflow in NEMeSIS","text":"<p>These jupyter notebooks contain a lot of descriptions to help guide the user already. Thus, to avoid repetition we will focus on their general aspects instead of detailing each step like it is done for the graphical interface.</p> <p>The larger jupyter notebook version of NEMeSIS is <code>NEMeSIS_Data_Analysis_V2.8.ipynb</code>. This notebook includes every step of the main steps of data analysis from the alignment of m/z peak lists representing samples to biological interpretation. Furthermore, an extra notebook regarding the conversion of raw spectral data in open mzML format to m/z peak lists is available in <code>Raw_Spectra_Processing.ipynb</code> (see here for details mzML Spectra Conversion). This was placed as a separate noteboook to lower memory necessity per individual notebook making the experience smoother. The interactive graphs with a very high number of points in <code>Raw_Spectra_Processing.ipynb</code> could affect performance in downstream steps.</p> <p>The main notebook is split into many different steps from step 0 to 11 which are described in the table of contents. Using this table is the fastest way to navigate the (quite large) notebook.</p> <p>Step 1 is the equivalent to Stage 1 of the graphical interface: Data Reading.</p> <p>Steps 1.1, 1.2, 1.3 and 2 are the equivalent to Stage 2 of the graphical interface: Data Pre-Processing and Pre-Treatment</p> <p>Steps 3 to 11 are the equivalent of Stage 3 of the graphical interface: Data Analysis and Biological Interpretation</p> <p>Finally, the program is capable of outputting trated data (Data Exporting) that can be used as input for other statistical analysis not present in the main NEMeSIS program in independent side modules (Independent Side Modules). These independent side modules are standalone jupyter notebooks that perform statistical analysis not present in the main notebook and can be adapted to specific statistical analysis desired by the researched. We present two independent side modules here that are, simultaneously, example side modules to be adapted by researchers to create others and that contain very useful analysis based on graph representations of samples built to consider possible biochemical transformations between metabolites - sMDiNs and FDiGNNs - to be performed based on the type of analyses desired - more information on these side modules here.</p> <p></p>"},{"location":"jupyter_docs/#definition-of-parameters","title":"Definition of Parameters","text":"<p>Looking at the first cell after <code>Step 1</code> shows the idea on how the jupyter notebook works.</p> <p>Each cell may have at its beginning a set of parameter that can be edited as shown below. These include the parameter name and the current parameter that can be edited. Usually, the parameter and what should be put there is explained in a comment after the parameter (or in the lines before if the explanation is large). In some cases, if the parameter only has a set number of options allowed, they are also enumerated in a comment.</p> <pre><code>    filename = '5yeasts_notnorm.csv' # Name of your file\n    ## Indicate if the file read includes the target (sample classes) in its first row\n    target_in_file = False\n    idx_masses = 'Neutral' # 'Neutral' (neutral masses), 'Positive' (Obtained from ESI+), 'Negative' (Obtained from ESI-)\n    # or 'None' (Neutral mass column cannot be inferred)\n</code></pre> <p>The rest of the cell then contributes to the current step of the analysis being performed. Most functions used in the data analysis are stored in the <code>metanalysis_standard.py</code> file while functions and code related to rendering figures or showing results are in the notebook itself to be more easily edited.</p>"},{"location":"jupyter_docs/#data-reading","title":"Data Reading","text":"<p>Info</p> <p>The first part of this section shares high similarities with the Stage 1: Data Input section of the Get started with GUI page.</p> <p>NEMeSIS accepts previosuly aligned 2D tables with samples on one axis and metabolic features on the other, samples represented as lists of m/z peaks or spectral raw data sample in mzML format. The first steps of the notebook is reading your input data, which requires strict formatting from the files. It can accept either data tables with samples having already been previously aligned or samples represented as lists of m/z peaks. The type of data to be used is chosen by choosing the <code>aligned_samples</code> parameter (first cell of Step 1) to <code>True</code> if data to be used is in form of data tables or <code>False</code> if it is lists of m/z peaks. For the former, you can then skip to the <code>Data Matrix (Aligned Samples) Reading Section</code> to define the filename and reading specific parameters while, for the latter, you go to the <code>Data Alignment Section</code> to define the filename and alignment specific parameters.</p> <p>For using spectral raw data sample in mzML format, the mzML files must be converted in a separate notebook capable of transforming these spectra into data tables and/or lists of m/z peaks to use as input for the main notebook. See mzML Spectra Conversion for more information.</p> <p>The allowed formatting for data tables is exemplified in the figure below with the addition that files can either be <code>.csv</code> or <code>.xlsx</code> files. Metabolic Features should be represented in the rows, while samples and metadata should be represented in columns. </p> <p>The first column in your data must correspond to the identifier of your metabolic features, more specifically, to a mass value if possible - black box - and they should be unique (non-repeating). The name of the column will be overwritten to be <code>Bucket label</code>. If the columns are mass values capable of being intepreted as floats (numbers), then a <code>Mass</code> column in your data will be added. This column is later necessary to perform <code>Data Annotation</code> (but nothing else). You have 3 possibilities to choose from: <code>Neutral</code> where the masses in your index are taken as the neutral masses creating a <code>Neutral Mass</code> column, <code>m/z (Positive)</code> or <code>m/z (Negative)</code> where the masses are assumed to be m/z values obtained in positive or negative (respectively) ionization mode creating a <code>Probable m/z</code> column.</p> <p>Danger</p> <p>Avoid having a column named <code>Neutral Mass</code> or <code>m/z</code> in your data, since it will be overwritten.</p> <p>The remaining columns are free to be any metadata you have in your data (green box) and the samples with the corresponding intensity values (purple box). If you are using LC-MS or GC-MS or any other kind of hyphenated method and have another column other than mass to characterize your features such as retention time, you can still analyse them and identify this columns as metadata but they will not be used for data annotation or any other step in the analysis.</p> <p>Optionally, the first row (after the column names) of your data may include the class labels of each of your samples (orange box) that will be used to automatically generate the target (class labels) later on.</p> <p></p> <p>The allowed formatting for lists of m/z peaks to be aligned is exemplified in the figure below. The software will expect a single Excel (<code>.xlsx</code> or <code>.xls</code>) file. It should have one sample per Excel sheet; the name of the Excel sheet should correspond to the sample name (orange box); each sheet should have in its first column the mass values (m/z, neutral mass or equivalent) and in its second column the corresponding intensity values; and finally, the first row should have the name of the two columns, for example, 'm/z' and 'I' (this name should be consistent between samples if possible). The example file <code>example_samples_to_align.xlsx</code> is available in the <code>Files_To_Align</code> folder as guidance.</p> <p></p> <p>Info</p> <p>The nature of the jupyter notebooks allows the researcher to tweak the data reading functions so they accept data in different formats as well.</p>"},{"location":"jupyter_docs/#figure-and-table-saving","title":"Figure and Table Saving","text":"<p>The jupyter notebook allows saving every table and figure made in the analysis. For most of the more relevant cases, this is already prepared to be performed, although the default option is turned off.</p> <p>For Figures, most have at the end of the code cell that produces something like the small code example shown below for the PCA Figure. By removing the '#' (comment), the figure will be saved when the cell is ran again (name can be adjusted).</p> <pre><code>#f.savefig('Name_PCAplot.png', dpi=600) # Save the figure\n</code></pre> <p>For the most significant Tables, they have usually a whole code cell to format and save the table like in the example shown below to save the list of important features to build Random Forest models. Here, at the beginning of the cell, there is usually a parameter than can be changed between <code>False</code> and <code>True</code> to save or not save the table. In the example shown, the parameter name is <code>SAVE_IMP_FEAT</code>. By changing it to <code>True</code>, the table would be saved. Another example is the <code>GENERATE_Excel_file</code> parameter in the <code>Common and Exclusive Compound Analysis</code> step.</p> <pre><code># Saving Important feature dataset in an excel\nSAVE_IMP_FEAT = False\n\n# Saving the most important features by their fraction 'frac_feat_impor'.\n# If None, saving the most important features based on a threshold 'VIP_Score_threshold'.\n# If also None, save the full dataset of all features\nfrac_feat_impor = 0.02 # Fraction of features to save, If None the variable in the next line is used.\nscore_threshold = None # Only used if variable above is None, threshold of score to consider a feature important.\n\nif SAVE_IMP_FEAT:\n    if frac_feat_impor:\n        max_idx = int(frac_feat_impor*len(imp_feats_rf))\n        filt_imp_feats_rf = imp_feats_rf.iloc[:max_idx]\n        filt_imp_feats_rf.to_excel(f'RF_ImpFeat_{frac_feat_impor*100}%.xlsx')\n    elif score_threshold:\n        filt_imp_feats_rf = imp_feats_rf[imp_feats_rf['Gini Importance'] &gt; score_threshold]\n        filt_imp_feats_rf.to_excel(f'RF_ImpFeat_GiniImpgreater{score_threshold}.xlsx')\n    else:\n        imp_feats_rf.to_excel(f'RF_FeatByImportance.xlsx')\n</code></pre>"},{"location":"jupyter_docs/#data-exporting","title":"Data Exporting","text":"<p>For exporting the data tables obtained after pre-processing and pre-treatment, a cell is available for saving different formats of the file in the NEMeSIS folder. This has the purpose to save data for the researcher to use somewhere else but also to be used as input for the Independent Side Modules. The multiple files are made because they can be useful in different contexts. Here, we show these files with their default name but this name can be changed in the same cell code mentioned. These are:</p> <ul> <li><code>Export_TreatedData.xlsx</code> - Excel with 4 sheets, one containing the normalized data (without missing value imputation, transformation or scaling) with metadata, another the treated intensity data without metadata, another with the dataset treated with Binary Simplification (or Spectral Digitalization) method and the last with the treated values after missing value imputation and normalization (but before transformation and scaling).</li> <li><code>Export_Target.txt</code> - File containing the class labels (target) of the dataset's samples in the same order as they appear in the dataset.</li> <li><code>Export_TreatedData.pickle</code> - Treated intensity data without metadata in pickle format to guarantee the 'Bucket Labels' suffer no changes (roundings).</li> <li><code>Export_ProcData.pickle</code> - Normalized data (without missing value imputation, transformation or scaling) with metadata in pickle format to guarantee the 'Bucket Labels' suffer no changes (roundings).</li> </ul> <p>Info</p> <p>The pickle versions of the dataset exist to eliminate the possiblity of rounding errors made to the index of the data tables, keeping them consistent between analyses.</p>"},{"location":"jupyter_docs/#mzml-spectra-conversion","title":"mzML Spectra Conversion","text":"<p>The mzML Spectra Conversion can be performed in an independent side jupytre notebook available in the NEMeSIS installation called <code>Raw_Spectra_Processing.ipynb</code>. This file can converts the open format mzML spectra into lists of m/z peaks that can be aligned into 2D data tables. For this purpose, the pyopenms Python package is used.</p> <p>The names of the file should be inputted in the 2nd code cell of the notebook in a list as exemplified in the notebook itself. The most critical parameter in <code>signal_to_noise</code> ratio can also be adjusted in the following cell. Furthermore, other parameters that can be adjusted are in the cell where the conversion is performed and with a comment as seen in code section below. To use these parameters, the # before each line can be removed and the parameter adjusted.</p> <pre><code>    param = cnt.getParameters()\n    param.setValue(\"signal_to_noise\", signal_to_noise)\n    #param.setValue(\"spacing_difference_gap\", 7.0)\n    #param.setValue(\"spacing_difference\", 5.0)\n    #param.setValue(\"SignalToNoise:win_len\", 200.0) # def 200\n    #param.setValue(\"SignalToNoise:bin_count\", 200) #def 200\n    #param.setValue(\"SignalToNoise:min_required_elements\", 100)\n    #param.setValue(\"SignalToNoise:max_intensity\", 1000000)\n    #param.setValue(\"SignalToNoise:auto_mode\", -1)\n</code></pre> <p>Explanations of each parameter are shown with extensive documentation is present in https://openms.de/current_doxygen/html/classOpenMS_1_1PeakPickerHiRes.html. Credit to pyopenms devs.</p> <p>The lists of m/z peaks can be directly exported to an excel in the format appropriate for the Data Alignment section of the main jupyter notebook. Furthermore, the next section can also performed the data alignment of the lists of m/z peaks equivalent to the main jupyter notebooks, that can also be exported to a <code>.csv</code> file that can be the input of the main jupyter notebook or the NEMeSIS GUI as well.</p>"},{"location":"jupyter_docs/#spectra-visualization","title":"Spectra Visualization","text":"<p>Info</p> <p>This section shares high similarities with the first part of the Spectra Visualization section of the Get started with GUI page.</p> <p>If files were succesfully converted and data alignment was performed, the last section allows to visualize and assess the quality of the spectral data processing by observing the raw, centroided (after spectral processing) and aligned (after data alignment) spectra in interactive graphs of a few samples simultaneously. They are interactive so close ups can be selected for each graph. This allows to observe the genral shape of peaks that were kept through the process and which were discarded and to see if the quality of the aligned spectra is as desired or expected.</p> <p>The image below shows an example. You can choose which samples to see, if the spectra values should be normalized (1), and which mass ranges to plot initially.</p> <ol> <li>Normalization in the raw, centroided and aligned spectra is made by dividing their intensities the sum of all intensities in the raw data specifically.</li> </ol> <p>Warning</p> <p>Spectra contain a very large number of points which combined to their interactive nature, make it heavy resource demanding for the software and the computer. Hence, although it depends on the computer available, we do not recommend selecting more than 3-4 samples simultaneously. Moreover, it may take a few seconds until the spectra are fully operational.</p> <p></p>"},{"location":"jupyter_docs/#independent-side-modules","title":"Independent Side Modules","text":"<p>The modular nature of the statistical analysis together with the Python implementation of NEMeSIS allows the addition of extra statistical analysis tailored to the specific dataset analysed by using independent side modules. These are illustrated how they can be applied by resorting to jupyter notebooks. To this end, NEMeSIS allows exporting data after the Data Pre-Processing and Pre-Treatment steps to be used by the independent modules. In order to facilitate the creation of these side modules, we present two side modules named <code>sMDiN_analysis_module.ipynb</code> and <code>FDiGNN_analysis_module.ipynb</code> that introduce how the data saved by the main workflows of NEMeSIS can be inputted and used.</p> <p>Moreover, these notebooks apply methodologies developed in the FT-ICR-MS-Lisboa laboratory group that we believe could improve and complement most analysis performed using the NEMeSIS software. The <code>sMDiN_analysis_module.ipynb</code> notebook applies the sample Mass-Difference Network (sMDiN) methodology as described in its paper here and the <code>FDiGNN_analysis_module.ipynb</code> notebook applied the Formula-Difference Graph Neural Network (FDiGNN) methodology as described here (put paper DOI when accepted) - currently submitted paper under review. Both methodologies focus on the representation of metabolomics as graph (non-tabular) representations instead of the 2D Data Tables usually used. These graph representations are built by taking into account possible chemical transformations between the detected metabolites, thus inserting into the data structural information that represents these possible transformations. The analysis performed will then include this implicit information stored in the graphs and use it to inform supervised models and metabolite importance, which can complement the conventional workflow. These methodolgoies are not included in the main NEMeSIS workflow since they are not a part of the conventional workflow most researchers start with to analyse data quality and because their use on the graphical interface would be more clunky as a consequence of the graph based approaches which could lead to a loss of nuance. Despite this, their complementarity and usefulness makes them indispensable to add as a side analysis to be performed.</p> <p>sMDiNs besides using a graph representation also use feature occurrence data instead of intensity data. The built a graph representing each sample that should be characteristic of each biological class and perform network analyis on the set of graphs constructed. The network analysis made is tied to the type of information that is desired to be extracted from the graphs (observe the performance of supervised models to see if the extracted data is reliable, that is, models with low performance cannot provide reliable information). For example, Weighted Mass-Difference based Building blocks Impact can be used to observe the difference in prevalence of different biochemical transformations used to build the graphs between classes - see paper here for details.</p> <p>FDiGNN builds a graph representation for each sample, adding attributes to the nodes (each representing a metabolite) such as intensity or feature occurrence. It then trains a Graph Neural Network model (with many parameters that can be adjusted and optimized) that can accept those graphs directly as input. The architecture of the network itself is also available to be altered and modified to the researcher's desires. By using PINNI (Probability-Impact based Network Node Importance), a measure of importance to each metabolite/node can be assigned. This prioritizes nodes near other nodes (in the network) that are important for discrimination. Thus, the methodology prioritizes the highlighting of network sections (associated metabolites by possible chemical transformations) over individual metabolite importance. Pathway analysis can also be performed to overlap this important sections to known pathways in order to improve biological interpretation. This unique emphasis on possible 'interactions' between metabolites is a great advantage of FDiGNNs. However, fitting neural networks can be computational heavy especially with larger networks which may lead to the model taking a few hours to be fit and information extracted.</p> <p>As for structure, the first few code cells in each case show how the exported data can be read back into the jupyter notebook obtaining all the main data tables (treated data, annotated data, BinSim treated data) as well as the target with the class labels of each sample. They also show the overall details that can be adjusted at the beginning, in these cases, the number of folds to perform stratified cross-validation and the number of iterations to repeat the cross-validation when fitting and estimating performance of supervised models. These parameters can change based on the type of analysis performed.</p> <p>After this section, each notebook contemplates the necessary procedures to apply their respective methodologies with detailed informations on the different steps present in the notebooks themselves. The <code>FDiGNN_analysis_module.ipynb</code> notebook has, at the end, a mini Dash application that will open on the browser to browse and analyse results from the FDiGNN analysis and metabolite importances extracted (as well as pathway analysis) facilitating interpretation.</p>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2025 FT-ICR and Structural MS lab</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>HMDB data is used under the Creative Commons (CC) Attribution-NonCommercial (NC) 4.0 International Licensing condition.</p> <p>LOTUS is released under the GNU General Public License v3.0. Copyright \u00a9 CC-BY-SA 2021.</p> <p>DrugBank data is used under the Creative Common\u2019s Attribution-NonCommercial 4.0 International License.</p> <p>The RaMP-DB is used under a GPL-2 license.</p> <p>venn.py file is used and available under 'The Unlicense' license.</p>"},{"location":"release_notes/","title":"Release Notes","text":"<p>This page includes the release notes for NEMeSIS (including also for the illustrative jupyter notebook independently).</p>"},{"location":"release_notes/#nemesis-graphical-interface-version-release-notes","title":"NEMeSIS Graphical Interface Version Release Notes","text":"<p>Version Beta.0 (09/09/2025)</p> <ul> <li>Added raw data spectra processing section to the DataAlignment specific interface.</li> <li>Improved Data Annotation where chosen databases are merged together into a complete database before perfoming annotations (made associated changes).</li> <li>Changed Formula Assignment to the score-based Formula Assignment. Small improvement on allowed mass deviation calculation.</li> <li>Re-structured pathway analysis section to select more conservative background sets (metabolites detected in the data) and 'significant' metabolites based on univariate or multivariate analysis.</li> <li>Added mapping formulas to pathway and pathway representation in pathways analysis section (only SMPDB pathways).</li> <li>Minor bugfixes and improvements in text.</li> </ul> <p>Version Alpha.5 (19/02/2025)</p> <ul> <li>Added possibility to select Quality Control (QC) samples.</li> <li>Added in-depth Data Filtering page with 4 main options: minimum appearance filters, intensity-based filters, QC based filters and variation based filters.</li> <li>Improved Peak De-Duplication (same annotation from different adducts are now merged by adding intensity values while same adduct peaks are merged by selecting the higher intensity value in each sample).</li> <li>Added data export to be used in independent jupyter notebook side modules.</li> <li>Added section on correlated metabolites to searched metabolite in Compound Finder page.</li> <li>Improved hover data for Volcano Plots.</li> <li>Added a few quality of life changes.</li> <li>Slight tweak to maximum file size accepted.</li> <li>Fixed normalization by a reference feature.</li> <li>Minor bugfixes.</li> </ul> <p>Version Alpha.4 (05/06/2024)</p> <ul> <li>Original files can now be read from different folders than where the software is (files will still be downloaded to the software folder).</li> <li>Added Formula Assignment Section to pipeline and allow use of these formulas in relevant section downstream.</li> <li>(Furthermore, added external jupyter notebook for creation of own formula database to use for this formula assignment).</li> <li>Added Pathway Over-Representation Analysis to the pipeline (requiring HMDB annotation).</li> <li>Simplifying some table displays and eliminating redundant parameters (decrease memory usage).</li> <li>Improved hover data for PCA and PLS-DA projection plots.</li> <li>Small bugfixes and enhancing descriptions.</li> <li>Added external interface software to perform Sample Peak Alignment (DataAlignmentInterface.py) whose output can be used as input to the software.</li> </ul> <p>Version Alpha.3 (26/04/2024)</p> <ul> <li>Added possibility to interpret mass values in data index as Neutral, Positive m/z or Negative m/z (in Data Reading).</li> <li>Added possibility to define which adducts to search for Data Annotation when comparing data masses to the databases selected.</li> <li>Mass calculations (of compounds based on formula and adducts) are now made with fractions instead of floats.</li> <li>Bugfix and improvement in PLS-DA components. Now maximum number of components based on number of samples available (and folds in cross-validation) is controlled.</li> <li>Updated the rest of the graphical interface and report generation based on the changes in Data Reading and adducts in Data Annotation.</li> <li>Minor bugfixes and improvements throughout software.</li> </ul> <p>Version Alpha.2 (20/02/2024)</p> <ul> <li>Added BinSim Analysis Page to the interface software (also included in report generation).</li> <li>Added possibility to save used data pre-processing, pre-treatment and data analysis parameters and to load them back in posterior analysis.</li> <li>Adapted classes and pages to work with possible save and loading of parameters. Attribute 'compute_fig' added to many classes allowing to cancel and restarting automatic figure updating.</li> <li>PCA now is not computed automatically when going into the data analysis section of the software.</li> <li>Bugfix in Univariate Analysis and change in how it is made (data filtering section within it). Now if data filtering used is 'total_samples', the filtering is performed by the percentage of samples allowed used with the full dataset rounded up. E.g. if the minimum n\u00ba of samples a feature must appear in a 15-sample dataset is 4, then by performing univariate analysis between 2 classes on a 6-sample subset, the minimum n\u00ba of samples allowed is (4/15) * 6 = 1.6 rounded up, that is, it has to appear in at least 2 of that 6 sample-subset (and 4 samples of the original 15).</li> <li>Multiple minor changes and improvements.</li> </ul> <p>Version Alpha.1 (30/12/2023)</p> <ul> <li>Added Report Generation feature and page to the interface software.</li> <li>Datasets up to 100 MB can now be read in the software (previously 20 MB was the maximum).</li> <li>Many bugfixes (especially in previously non-considered fringe cases), improvements and updates to handling page layouts, saving current parameters used for different analysis, improved image and table filenames (that could be saved with incorrect parameters in the name), better description and organization of different variables, to the software reset and soft-reset (in case of change in pre-treatments) procedures.</li> </ul> <p>Version Alpha.0 (13/12/2023)</p> <ul> <li>Alpha Version of the Graphical Interface of the NEMeSIS software (includes homepage, instruction page and all data analysis pipeline except for BinSim Analysis).</li> </ul> <p></p>"},{"location":"release_notes/#jupyter-notebook-version-release-notes","title":"Jupyter Notebook Version Release Notes","text":"<p>V2.8 - Raw Data Processing, Improved Data Annotation, Pathway Analysis and FDiGNN side module (09/09/2025)</p> <ul> <li>Added raw data spectra processing as a side module to process mzML data.</li> <li>Improved Data Annotation where chosen databases are merged together into a complete database before perfoming annotations (made associated changes).</li> <li>Small improvement on allowed mass deviation calculation for formula assignment.</li> <li>Improved intersection plot section and adding intersection plot colouring chemical compound classes.</li> <li>Re-structured pathway analysis section to select more conservative background sets (metabolites detected in the data) and 'significant' metabolites based on univariate or multivariate analysis.</li> <li>Added mapping formulas to pathway and pathway representation in pathways analysis section (only SMPDB pathways).</li> <li>Added a side module with FDiGNN analysis as a standalone jupyter notebook as example of independent analysis modules.</li> <li>Minor bugfixes and improvements in text.</li> </ul> <p>V2.7 - Data Filtering, Score-based Formula Assignment, Improved Peak De-duplication (19/02/2025)</p> <ul> <li>Added possibility to select Quality Control (QC) samples.</li> <li>Added in-depth Data Filtering options of 4 main types: minimum appearance filters, intensity-based filters, QC based filters and variation based filters.</li> <li>Added Score-based sections to Formula Assignment algorithm and slight changes in how the formula database is used.</li> <li>Improved Peak De-Duplication (same annotation from different adducts are now merged by adding intensity values while same adduct peaks are merged by selecting the higher intensity value in each sample).</li> <li>Added data export to be used in independent side modules.</li> <li>Added random_state to define cross-validation splits for RF and PLS-DA models (only for splits).</li> <li>Added a side module with sMDiN analysis as a standalone jupyter notebook as example of independent analysis modules.</li> <li>Added a few quality of life changes.</li> <li>Fixed normalization by a reference feature.</li> <li>Minor bugfixes.</li> </ul> <p>V2.6 - Introducing Peak Alignment, Formula Assignment, Pathway Over-Representation Analysis and expansion of compound finder tools (05/06/2024)</p> <ul> <li>Added Sample Peak Alignment to Data Reading section of the notebook as an alternative to directly reading pre-aligned files.</li> <li>Added Formula Assignment Section to pipeline and allow use of these formulas in relevant section downstream.</li> <li>(Furthermore, addded external jupyter notebook for creation of own formula database to use for this formula assignment).</li> <li>Added Pathway Over-Representation Analysis to the pipeline (requiring HMDB annotation).</li> <li>Added option to search for KEGG ID in finder (requires HMDB annotation).</li> <li>Added search for highly positive or negative correlated metabolites to a metabolite of choice.</li> <li>Added option to search for compounds in batch (requires external file).</li> <li>Small improvements in code and descriptions as well as small bugfixes.</li> </ul> <p>V2.5 - Introducing adducts to data annotation and expansion of univariate analysis (23/04/2024).</p> <ul> <li>Type of index in the dataset can be chosen if they are positive/negative m/z masses or neutral masses, creating respectively a 'Probable m/z' or a 'Neutral Mass' column.</li> <li>Added way to introduce which adduct to consider for data annotation.</li> <li>Data Annotation logic slightly changed to account for adducts and calculate adduct masses on the databases chosen and match them to the dataset masses. Added arg for possiblity to only select the candidates with the lowest ppm deviation to a peak</li> <li>Minor improvements and changes to accomodate new data annotation in <code>duplicate_disambiguator</code> and <code>individually_merging</code> functions.</li> <li>Minor change in supervised analysis related function with the arg <code>regres</code> or <code>regression</code>.</li> <li>Added explanation for maximum number of components to choose in PLS-DA.</li> <li>XGBoost will now be skipped by default for binary classifications.</li> <li>In 2-Class Univariate Analysis, change to calculate the minimum number of samples to filter the subset of data by the <code>total_samples</code> method when using a more than 2-Class dataset.</li> <li>Added multi-class univariate statisical analysis (ANOVA and Kruskal-Wallis test).</li> <li>Add 'index' as a search possibility in the compound finder and minor bugfix where previously annotated columns would not be searched when using 'Name' or 'Formula' options.</li> <li>Improve to some descriptions and minor bugfixes.</li> </ul> <p>V2.4 - XGBoost, Regressions and KEGG colouring (02/02/2024)</p> <ul> <li>Added regression Random Forest analysis and PLS.</li> <li>Added XGBoost analysis.</li> <li>Added KEGG colour mapping.</li> <li>Small bugfix in index hyperlinks when using Visual Studio Code (did not jump to subsections).</li> <li>Example data in De-duplication section is now skipped if the example dataset is not loaded.</li> </ul> <p>V2.3 - Data Diversity Plot Improvements and Other Minor Changes (14/12/2023)</p> <ul> <li>Bugfix in Kendrick Mass Defect plots, chemical composition series plots and PCA of chemical composition series where using multiple database formula annotations would only include formulas assigned in all chosen databases instead of in at least one of them.</li> <li>Change in Kendrick Mass Defect plots allowing points to be coloured by chemical composition series based on database formula annotation performed in the notebook instead of only being able if formulas came from MetaboScape 'SmartFormula'.</li> <li>Improved Data Diversity Plots section.</li> <li>Small bugfix in compound finder search tool when looking for m/z values, finder DataFrame would not update.</li> <li>Changed RAMP ID pathway database to the improved version in HMDB Compound Pathway Assignment section.</li> <li>Updated type of seaborn template for current seaborn versions.</li> </ul> <p>V2.2 - HMDB Compound Pathway Assignment (17/11/2023)</p> <ul> <li>Added possibility of reading target directly from the excel or csv file with your data (changed initial filtering place to account for this).</li> <li>Added section that allows to see pathways related to HMDB compounds annotated.</li> <li>Fixed 'Normalized Intensity' position (y axis) in compound finder plots.</li> <li>Fixed deprecation issue in Kendrick Mass Defect plots.</li> </ul> <p>V2.1 - Permutation Tests, Volcano Plots and Loadings in PCA (06/06/2023)</p> <ul> <li>Updated and added PCA function to metanalysis.py (now retrieves loadings).</li> <li>Added Permutation Testing for Random Forest and PLS-DA supervised analysis that can accept one of multiple metrics.</li> <li>Added ROC Curve for PLS-DA and updated ROC section for Random Forest in notebook.</li> <li>Added Volcano Plot prototype in visualization of unsupervised analysis.</li> <li>Added PCA and Loading plot (biplot) based on the counts of each chemical composition series in each sample.</li> <li>Some minor changes and additions in comments.</li> </ul> <p>V2.02 - Git-hub Repository (17/02/2023)</p> <ul> <li>Changes to put the project in a git-hub repository (patch notes now in Readme.md file of repository).</li> <li>Added Readme.md and requirements.txt file.</li> <li>Updated package installation instructions.</li> <li>Changed HMDB database to be a xlsx file (smaller file) and minor bugfix in database reading.</li> </ul> <p>V2.01 - Inserted references (09/02/2023)</p> <ul> <li>Put clear references to the sources used for the different functions and steps.</li> <li>Minor changes.</li> </ul> <p>V2.0 - Reorganization and merging of notebooks, addition of multiple quality of life features, univariate analysis, etc. (08/02/2023)</p> <ul> <li>Reorganization of notebook for a more streamlined and easy-to-understand experience.</li> <li>Merged the two notebooks based on if you had performed metabolite annotation on MetaboScape into one.</li> <li>Put all essential functions in a separate file - <code>metanalysis_standard.py</code> - to de-clutter notebook. Recommended to check this file too see better the options available for each.</li> <li>Updated Step 0 instructions to install metabolinks and other packages.</li> <li>Added way to annotate your data with multiple databases in the notebook (a set of 4 columns for each database) and remade annotation section.</li> <li>Added way to check for duplicate (or more) annotations and remove those when possible by specific procedures - step 1.3.</li> <li>Changed default imputation method to 'min_sample' from 'min_feat'.</li> <li>Fixed bug in 'extra_filt' that could be made during <code>basic_feat_filtering</code>.</li> <li>Venn Diagram now follows each classes' colours as set up during step 1.1.</li> <li>Increased the number of options available to plot UpSetPlot including showing the number of annotated compounds.</li> <li>Slight change in ROC curve calculation and plotting and bugfix.</li> <li>Added more meta_data to important features tables from Random Forest and PLS-DA and allowed output of important feature tables.</li> <li>Added Univariate Analysis and Fold Change analysis - Step 6.</li> <li>Made points colored in the Van Krevelen based on either the log of their average intensity or the rank of their average intensity (more useful).</li> <li>Made Chemical Composition Series appear for all classes in the same plot (more convenient to compare).</li> <li>Added Kendrick Mass Defect plots to step 7.</li> <li>Added a BinSim specific step for Unsupervised and Supervised analysis.</li> <li>Made the compound finder more universal accepting formulas, m/z or Names. Added boxplot and average bar graph to the usual bar graph in this section.</li> <li>Minor typo fixes and added explanations.</li> </ul> <p>V1.4 - Improvements to common and exclusive features analysis (03/01/2023)</p> <ul> <li>Added automatic Venn Diagram plotting for common and exclusive features analysis (venn.py must now be in the same folder)</li> <li>Added automatic UpSet plot construction (an alternative to Venn Diagram) which may be preferable for larger numbers of classes (may require installing a new package - pip install upsetplot)</li> </ul> <p>V1.3 - Filtering_pretreatment function and PLS-DA analysis metrics bug fixes (06/12/2022)</p> <ul> <li>Fixed multiple bugs in <code>filtering_pretreatment</code> function where args for filt_method, filt_kw, extra_filt, extra_filt_kw, scaling and scaling_kw would only use the default values even if the user changed it in the function.</li> <li>Fixed issue where calculation of f1-score, precision and recall metrics of PLS-DA would fail if the number of samples per class was not a multiple of the number of folds in cross validation.</li> </ul> <p>V1.2 - Quality of Life Improvements and Minor Corrections (03/11/2022)</p> <ul> <li>Added Table of Contents for quick navigation of the different steps in the notebook (also allows further analysis to be added to notebooks without making it clunkier and more difficult to navigate).</li> <li>Added Patch Notes to end of notebook for user to follow changes on notebook.</li> <li>Correcting bug in Random Forest and PLS-DA feature importance workflow mentioned in V1.1 for the PythonAnnotation Notebook.</li> <li>Added feature to step 5 by adding a way to see common and exclusive names/formulas as well as overall compounds/masses. Use id_selection to choose either 'Name' ('Matched names') or 'Formula' ('Matched formulas').</li> <li>Other minor changes and corrections.</li> </ul> <p>V1.1 - Feature Importance Correction (14/10/2022)</p> <ul> <li>Correcting bug in Random Forest and PLS-DA feature importance workflow that would not give the correct names from the dataframe to the most important features (it would give the first names in the dataset instead of the important ones).</li> <li>Updating instructions for Metabolinks installation.</li> </ul> <p>V1.0 - Release Version (03/10/2022)</p> <ul> <li>First release version of the metabolomics data analysis notebooks.</li> </ul>"}]}
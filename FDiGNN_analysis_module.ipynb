{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869f8d1a",
   "metadata": {},
   "source": [
    "# Extra Module for Metabolomics Data Analysis\n",
    "\n",
    "As a side module, all related functions to it are also present in the notebook.\n",
    "\n",
    "# FDiGNN Analysis (Formula-Difference Graph Neural Networks Analysis)\n",
    "\n",
    "FDiGNN or Formula-Difference Graph Neural Networks is an alternative way to process and train a supervised model of the data to complement the usual workflow that is available in this separated module as a jupyter notebook.\n",
    "\n",
    "First the method, changes the tabular metabolomics data into a graph structure called a Formula-Difference Network or FDiNs. The FDiNs use the assigned formulas as nodes in a network that are linked if their difference corresponds to a common biochemical transformation. This FDiN is used as a basis for every sample which add their node features (mass, intensity and feature occurrence in the sample) and edge weight (2 if the transformation is observed in the SMPDB, 1 otherwise). **The chemical transformations considered will determine the structure of the FDiN.**\n",
    "\n",
    "These FDiNs are used as input for a deep learning methodology called Graph Neural Network (GNN), hence the denomination FDiGNN. This approach is **highly tuneable**. We present a model architecture absed on TAG layers, global attention pooling and classifier layers but this can be change. Furthermore, the model needs to be optimized for each dataset by changing hyperparameters such as: **number of TAG layers**, **hidden channels in each layer**, **learning rate** (rate, weight decay and exponential decay gamma), **dropout probability on dropout**, **number of epochs trained** or **batch size**.\n",
    "\n",
    "This module will allow to obtain an estimation of the model performance, obtain the node/metabolite importances and perform network and pathway enrichment analysis to analyse the data.\n",
    "\n",
    "**Estimating model performance and calculating feature importance may take from a few minutes to a few hours depending on the size of the dataset.**\n",
    "\n",
    "## Extra Installations\n",
    "\n",
    "To fully run this, it is required to install a few extra Python packages over the usual software. These are:\n",
    "\n",
    "- pytorch (see https://pytorch.org/ to see how to install based on your computer)\n",
    "- pytorch geometric (see https://pytorch-geometric.readthedocs.io/en/2.6.1/install/installation.html to see how to install based on your computer)\n",
    "- dash (if not installed, install by running 'conda install conda-forge::dash')\n",
    "\n",
    "\n",
    "paper doi: **To Put**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# scientific python imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# metabolinks and \"in folder\" modules\n",
    "import metabolinks.transformations as transf\n",
    "import metanalysis_standard as metsta\n",
    "\n",
    "import MDiN_functions as md\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, Softmax, Sequential, BatchNorm1d, ReLU, Dropout, LeakyReLU\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATv2Conv, TAGConv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool\n",
    "\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "# For Dash App\n",
    "import networkx as nx\n",
    "import dash_cytoscape as cyto\n",
    "import dash\n",
    "from dash import Dash, dcc, html, Input, Output, ctx, callback\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Report versions\n",
    "print(\"PyTorch version\", torch.__version__)\n",
    "print(\"CUDA version\", torch.version.cuda)\n",
    "\n",
    "torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'\\nUSING DEVICE {device}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b4bce",
   "metadata": {},
   "source": [
    "#### Import treated Data from the main notebook.\n",
    "\n",
    "Select the filename of the file to import from the main jupyter notebook analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d807750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename for the data to import\n",
    "filename = 'Export_TreatedData.xlsx'\n",
    "filename_pickle_treated = 'Export_TreatedData.pickle'\n",
    "filename_pickle_proc = 'Export_ProcData.pickle'\n",
    "\n",
    "#treated_data = pd.read_excel(filename, sheet_name='Fully Treated Data').set_index('Unnamed: 0').T\n",
    "bin_data = pd.read_excel(filename, sheet_name='BinSim Treated Data').set_index('Probable m/z').T\n",
    "univariate_data = pd.read_excel(filename, sheet_name='MVI+Norm Data').set_index('Unnamed: 0')\n",
    "\n",
    "processed_data = pd.read_pickle(filename_pickle_proc)\n",
    "treated_data = pd.read_pickle(filename_pickle_treated)\n",
    "bin_data.columns = treated_data.columns\n",
    "\n",
    "sample_cols = treated_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename for the target to import\n",
    "filename_target = 'Export_Target.txt'\n",
    "\n",
    "with open(filename_target) as a:\n",
    "    tg = a.readlines()\n",
    "target = [t.strip() for t in tg]\n",
    "classes = pd.unique(pd.Series(target))\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f7d06",
   "metadata": {},
   "source": [
    "## Details for the Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213960d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 3\n",
    "iter_num = 10\n",
    "\n",
    "colours = sns.color_palette('tab10', 10) # Set the colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9611b8-72e9-4090-8970-3512db43fe04",
   "metadata": {},
   "source": [
    "# Formula-Difference Network (FDiN)\n",
    "\n",
    "Here, we built the FDiN for the whole dataset.\n",
    "\n",
    "This network is built from assigned formulas either from data annotation or formula assignment in the data. The FDiN has formulas representing metabolic peaks as nodes which are linked between each other if the difference in their formulas corresponds to one of the biochemical transformations chosen to be followed.\n",
    "\n",
    "Futhermore, the built FDiN might be supplemented with knowledge based edges if the parameter `add_knowledge_based_edges` is set to True. If so, a metabolic knowledge network encompassing all reactions between HMDB compounds represented by their formula stored in the **metabolic** pathways of the **SMPDB** database is used. The reactions were translated by linking every reactant to every product. See details of the construction of this knowledge-based metabolic network here: **LINK TO PAPER**.\n",
    "\n",
    "Here, if there is reaction in the database linking two formulas that are in the studied dataset, then that edge is also added to our FDiN even if their difference does not correspond to one of the chemical transformations chosen to be followed. Any edge that can be mapped to this network gets an edge weight of 2 compared to 1 of usual links.\n",
    "\n",
    "## Prepare the Network\n",
    "\n",
    "We prepared the network to accept the annotations that we perform in our software, however it can take others. In general we create two parameters: `prioritized_ann_col` and `base_form_assign_col` which represent the column from the annotation and the column from the formula assignment. As a reminder, the annotation we perform annotates all possible compounds to a peak as a list.\n",
    "\n",
    "In general, this will prioritize formulas from the annotation overwriting the ones from the formula assignment if there is only one formula assigned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prioritized_ann_col = 'Matched formulas'\n",
    "base_form_assign_col = 'Formula_Assignment'\n",
    "\n",
    "new_col_name = 'Forms_to_use'\n",
    "\n",
    "add_knowledge_based_edges = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of formulas to build the sFDiN\n",
    "temp_df = processed_data.copy()\n",
    "for i in temp_df.index:\n",
    "    # See formulas of annotated compounts\n",
    "    fs = temp_df.loc[i, prioritized_ann_col]\n",
    "    if type(fs) == list:\n",
    "        fs = list(set(fs))\n",
    "        # If only one annotation, overwrite the formula with it\n",
    "        if len(fs) == 1:\n",
    "            temp_df.loc[i, 'Formula_Assignment'] = fs[0]\n",
    "            # Should've been 'Matched HMDB adducts' here, did not catch that before\n",
    "            temp_df.loc[i, 'Formula_Assignment Adduct'] = temp_df.loc[i, 'Matched formulas'][0]\n",
    "        # If more than one annotation\n",
    "        else:\n",
    "            counted = False\n",
    "            # If any of the annotated formulas is equal to the formula assigned, change it and keep it\n",
    "            for f in fs:\n",
    "                if f == temp_df.loc[i, 'Formula_Assignment']:\n",
    "                    counted = True\n",
    "            if counted == False:\n",
    "                new_f = []\n",
    "                # See formulas that only have C, H, O, N, S, P elements while having 1 C and 1 H at least\n",
    "                for f in fs:\n",
    "                    a = md.formula_process(f)\n",
    "                    #print(a)\n",
    "                    if a['C'] != 0 and a['H'] != 0:\n",
    "                        if len(a) == 8:\n",
    "                            if a['Cl'] == 0 and a['F'] == 0:\n",
    "                                #print('hmm')\n",
    "                                new_f.append(f)\n",
    "                # If only 1 formula is in this conditions, overwrite formula assignment\n",
    "                if len(new_f) == 1:\n",
    "                    temp_df.loc[i, 'Formula_Assignment'] = new_f[0]\n",
    "                # If more than 1 formula is in these conditions, see these rare exceptions\n",
    "                else:\n",
    "                    print('---')\n",
    "                    print(len(new_f))\n",
    "                    print(new_f)\n",
    "                    print(temp_df.loc[i, 'Formula_Assignment'])\n",
    "                    print(i)\n",
    "                    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6439e2",
   "metadata": {},
   "source": [
    "Put Formulas to build the FDiN in DataFrame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a753ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_df = temp_df\n",
    "# Get the formulas from formula assignment, excluding isotopes\n",
    "formula_df = formula_df.dropna(subset='Formula_Assignment')\n",
    "formula_df = formula_df.loc[[i for i in formula_df.index if 'iso.' not in formula_df.loc[i, 'Formula_Assignment']]]\n",
    "# Add the counts of the different elements in columns\n",
    "elems = metsta.create_element_counts(formula_df, formula_subset=['Formula_Assignment',], compute_ratios=False,\n",
    "                                     drop_duplicates=False)\n",
    "filt_elems = elems.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb171e2a",
   "metadata": {},
   "source": [
    "Choose the list of Mass-Difference-based Building blocks (MDBs) to use and get them to DataFrame format\n",
    "\n",
    "MDBs are the list of chemical transformations that are used to build Mass-Difference Networks. They usually represent some of the most common and ubiquitous reactions in biological systems but can also be specific to the biological system in case. These 15 were the same as used in the FDiGNN paper.\n",
    "\n",
    "More MDBs can be added or removed from the list in the 2nd line of the cell below based on what chemical transformations want to be followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fda91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepted chemical transformations\n",
    "MDB = ['H2','CH2','CO2','O','CH2O','NCH','O(N-H-)','S','CONH','PO3H','NH3(O-)','SO3','CO', 'C2H2O', 'H2O']\n",
    "\n",
    "# Create MDB list\n",
    "results = {}\n",
    "for i in MDB:\n",
    "    results[i] = md.formula_process(i, elems=filt_elems.columns)\n",
    "MDB_df = pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ca81e",
   "metadata": {},
   "source": [
    "Read the built metabolic knowledge network and keep only the formula nodes that are present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SMPDB_MetaNetwork_general.pickle', 'rb') as f:\n",
    "    FDiN_knowledge = pickle.load(f)\n",
    "node_list = list(FDiN_knowledge.nodes())\n",
    "\n",
    "\n",
    "# See which of these formulas were detected in our dataset\n",
    "keep_idxs = []\n",
    "keep_formulas = []\n",
    "keep_pathways = []\n",
    "form_to_idx = {} # Dictionary so we know the association between formulas and idxs\n",
    "\n",
    "for i in temp_df.index:\n",
    "    counted = False\n",
    "    # Give priority to annotated formulas\n",
    "    fs = temp_df.loc[i, 'Matched formulas']\n",
    "    if type(fs) == list:\n",
    "        fs = list(set(fs))\n",
    "        # Only 1 Formula assigned in annotated data\n",
    "        if len(fs) == 1:\n",
    "            form = fs[0]\n",
    "            # If the formula is the knowledge network\n",
    "            if form in node_list:\n",
    "                keep_idxs.append(i)\n",
    "                keep_formulas.append(form)\n",
    "                if form in form_to_idx:\n",
    "                    form_to_idx[form].append(i)\n",
    "                else:\n",
    "                    form_to_idx[form] = [i,]\n",
    "                    keep_pathways.extend(FDiN_knowledge.nodes()[form]['Pathways'])\n",
    "                counted = True\n",
    "        # More than 1 Formula assigned in annotated data\n",
    "        else:\n",
    "            form_in_node_list = []\n",
    "            for f in fs:\n",
    "                if f in node_list:\n",
    "                    form_in_node_list.append(f)\n",
    "            if len(form_in_node_list) >= 1:\n",
    "                for f in form_in_node_list:\n",
    "                    keep_idxs.append(i)\n",
    "                    keep_formulas.append(f)\n",
    "                    if f in form_to_idx:\n",
    "                        form_to_idx[f].append(i)\n",
    "                    else:\n",
    "                        form_to_idx[f] = [i,]\n",
    "                        keep_pathways.extend(FDiN_knowledge.nodes()[f]['Pathways'])\n",
    "                counted = True\n",
    "\n",
    "    # Formula assignment Formulas\n",
    "    if not counted:\n",
    "        fs = temp_df.loc[i, 'Formula_Assignment']\n",
    "        if type(fs) == str:\n",
    "            if fs in node_list:\n",
    "                keep_idxs.append(i)\n",
    "                keep_formulas.append(fs)\n",
    "                if fs in form_to_idx:\n",
    "                    form_to_idx[fs].append(i)\n",
    "                else:\n",
    "                    form_to_idx[fs] = [i,]\n",
    "                    keep_pathways.extend(FDiN_knowledge.nodes()[fs]['Pathways'])\n",
    "\n",
    "# Subgraph the FDiN to only keep these formulas as information for FDiGNN\n",
    "FDiN_knowledge = FDiN_knowledge.subgraph(keep_formulas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cedd4",
   "metadata": {},
   "source": [
    "## Building the FDiN proper\n",
    "\n",
    "General FDiN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46834049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FDiN basis\n",
    "FDiN = nx.Graph()\n",
    "FDiN.add_nodes_from(filt_elems.index) # Each formula is a node\n",
    "# Adding relevant attributes\n",
    "nx.set_node_attributes(FDiN, formula_df['Formula_Assignment'].to_dict(), name='Formula')\n",
    "\n",
    "# Adding simple edges from metabolic transformations\n",
    "for formula in filt_elems.index:\n",
    "    poss_formulas = filt_elems.loc[formula] + MDB_df\n",
    "    for i in poss_formulas.index:\n",
    "        poss_matches = filt_elems[(filt_elems == poss_formulas.loc[i]).sum(axis=1) == len(MDB_df.columns)]\n",
    "        for node in poss_matches.index:\n",
    "            FDiN.add_edge(formula, node, Transformation=i, Weight=1)\n",
    "\n",
    "if add_knowledge_based_edges:\n",
    "    # Adding Knowledge-based edges from the metabolic-knowledge based network\n",
    "    for n1 in FDiN.nodes():\n",
    "        formA = FDiN.nodes()[n1]['Formula']\n",
    "        if formA in FDiN_knowledge.nodes():\n",
    "            for n2 in FDiN.nodes():\n",
    "                formB = FDiN.nodes()[n2]['Formula']\n",
    "                if formA != formB:\n",
    "                    if formB in FDiN_knowledge.nodes():\n",
    "                        if (formA, formB) in FDiN_knowledge.edges():\n",
    "                            if (n1, n2) in FDiN.edges():\n",
    "                                # Change edge weight if the edge already existed to 2\n",
    "                                FDiN.edges()[(n1, n2)]['Weight'] = 2\n",
    "                            else:\n",
    "                                FDiN.add_edge(n1, n2, Transformation='Knowledge', Weight=2)\n",
    "                        # Confirm nothing is being lost\n",
    "                        elif (formB, formA) in FDiN_knowledge.edges():\n",
    "                            print('-------')\n",
    "\n",
    "print(f'Total with: {len(FDiN.edges())} edges.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dc47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Sizes of components\n",
    "print('Size of components in the FDiN:')\n",
    "[len(i) for i in sorted(nx.connected_components(FDiN), key=len, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a25421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose minimum component size to use\n",
    "min_comp_size = 7\n",
    "\n",
    "# Filter FDiN to only include larger components\n",
    "comps = []\n",
    "for i in sorted(nx.connected_components(FDiN), key=len, reverse=True):\n",
    "    if len(i) > min_comp_size:\n",
    "        comps.extend(i)\n",
    "FDiN = FDiN.subgraph(comps)\n",
    "print(f'Filtered FDiN with: {len(FDiN.edges())} edges.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643556b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save graph object to file\n",
    "#pickle.dump(FDiN, open('FDiN_Dataset.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017e421",
   "metadata": {},
   "source": [
    "## Building the sample Formula-Difference Networks\n",
    "\n",
    "For each sample, copy the FDiN and add the following node features:\n",
    "\n",
    "- Peak Mass (divided by 100 so it has smaller values)\n",
    "- (Treated) Intensity\n",
    "- Feature Occurrence (1 or 0)\n",
    "\n",
    "Other node features can be added as well here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c97aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mass Column\n",
    "mass_col = 'Neutral Mass' if 'Neutral Mass' in processed_data.columns else 'Probable m/z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "sFDiNs_full = {}\n",
    "for samp in sample_cols:\n",
    "\n",
    "    sFDiNs_full[samp] = FDiN.copy()\n",
    "\n",
    "    # Intensity feature\n",
    "    ints = {i: treated_data.loc[samp, i] for i in formula_df.index}\n",
    "    # Feature Occurrence feature\n",
    "    pres = {i: bin_data.loc[samp, i] for i in formula_df.index}\n",
    "\n",
    "    # Storing intensity of feature in sample, mass and feature occurrence on the nodes\n",
    "    intensity_attr = dict.fromkeys(sFDiNs_full[samp].nodes(),0)\n",
    "    for m in sFDiNs_full[samp].nodes():\n",
    "        intensity_attr[m] = {'mass':formula_df.loc[m,mass_col]/100, 'intensity': ints[m], 'presence':pres[m]}\n",
    "    nx.set_node_attributes(sFDiNs_full[samp], intensity_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d6096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the sFDiNs built\n",
    "print('Full sFDiNs')\n",
    "print('Number of nodes:', len(sFDiNs_full[samp].nodes()))\n",
    "print('Number of edges:', len(sFDiNs_full[samp].edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7e3f1",
   "metadata": {},
   "source": [
    "### Passing FDiNs to the pytorch architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Features\n",
    "print('Nº of node features excluding the string formula:',\n",
    "      len(list(sFDiNs_full[samp].nodes()[list(sFDiNs_full[samp].nodes())[0]].keys())[1:]))\n",
    "node_attrs = list(sFDiNs_full[samp].nodes()[list(sFDiNs_full[samp].nodes())[0]].keys())[1:]\n",
    "\n",
    "# Edge Attributes\n",
    "edge_attrs = list(sFDiNs_full[samp].edges()[list(sFDiNs_full[samp].edges())[0]].keys())\n",
    "edge_attrs.remove('Transformation')\n",
    "\n",
    "print('Nº of node features:', len(node_attrs))\n",
    "print('Nº of edge features:', len(edge_attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sFDiNs into PyTorch geometric\n",
    "data_list_full = []\n",
    "for samp in sFDiNs_full:\n",
    "    pyg_graph = from_networkx(sFDiNs_full[samp],\n",
    "                              group_node_attrs=list(\n",
    "                                  sFDiNs_full[samp].nodes()[list(sFDiNs_full[samp].nodes())[0]].keys())[1:],\n",
    "                              group_edge_attrs=edge_attrs)\n",
    "    data_list_full.append(pyg_graph.to(device))\n",
    "dataset = DataLoader(data_list_full)\n",
    "\n",
    "# Definition of class depends if there are 2 classes or more\n",
    "if len(classes) == 2:\n",
    "    # Adding target information to the sFDiNs\n",
    "    for g in range(len(target)):\n",
    "        if target[g] == classes[0]:\n",
    "            data_list_full[g].y = torch.FloatTensor([1]).type(torch.LongTensor).to(device)\n",
    "        else:\n",
    "            data_list_full[g].y = torch.FloatTensor([0]).type(torch.LongTensor).to(device)\n",
    "\n",
    "else:\n",
    "    onehot = pd.get_dummies(target, dtype=int)\n",
    "    for g in range(len(target)):\n",
    "        data_list_full[g].y = torch.FloatTensor(onehot.iloc[g].values).type(torch.LongTensor).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cc5976",
   "metadata": {},
   "source": [
    "# Setting up the FDiGNN Model\n",
    "\n",
    "Global attention pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa322a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAttentionPooling(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(GlobalAttentionPooling, self).__init__()\n",
    "        self.attention_nn = nn.Sequential(nn.Linear(in_channels, 1), nn.Sigmoid())\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.last_scores = None\n",
    "        self.x_weighted = None\n",
    "        \n",
    "    def forward(self, x, batch):\n",
    "        scores = self.attention_nn(x).squeeze(-1)\n",
    "        scores = softmax(scores, batch)\n",
    "        x_weighted = x * scores.unsqueeze(-1)\n",
    "        self.last_scores = scores\n",
    "        self.x_weighted = x_weighted\n",
    "        graph_embedding = scatter_add(x_weighted, batch, dim=0)\n",
    "\n",
    "        return graph_embedding\n",
    "\n",
    "    def get_attention_scores(self):\n",
    "        return self.last_scores, self.x_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1723cc6",
   "metadata": {},
   "source": [
    "General Model - 4TAG_AttPool_2Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the model\n",
    "class FDiGNN_TAG(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, drop, n_node_feat, K, n_classes):\n",
    "        super(FDiGNN_TAG, self).__init__()\n",
    "        torch.manual_seed(89356)\n",
    "        self.conv1 = TAGConv(n_node_feat, hidden_channels, K=K)\n",
    "        self.norm1 = BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = TAGConv(hidden_channels, hidden_channels, K=K)\n",
    "        self.norm2 = BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = TAGConv(hidden_channels, hidden_channels, K=K)\n",
    "        self.norm3 = BatchNorm1d(hidden_channels)\n",
    "        self.conv4 = TAGConv(hidden_channels, hidden_channels, K=K)\n",
    "        self.norm4 = BatchNorm1d(hidden_channels)\n",
    "        self.pooling = GlobalAttentionPooling(hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, n_classes)\n",
    "        self.drop = drop\n",
    "\n",
    "        self.leakyrelu1 = nn.LeakyReLU()\n",
    "        self.leakyrelu2 = nn.LeakyReLU()\n",
    "        self.leakyrelu3 = nn.LeakyReLU()\n",
    "        self.leakyrelu4 = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight):\n",
    "        # 1. Obtain node embeddings \n",
    "        x1 = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x1_relu = self.leakyrelu1(x1)\n",
    "        x1_norm = self.norm1(x1_relu)\n",
    "        x1_drop = F.dropout(x1_norm, p=self.drop, training=self.training)\n",
    "\n",
    "        x2 = self.conv2(x1_drop, edge_index, edge_weight=edge_weight)\n",
    "        x2_relu = self.leakyrelu2(x2)\n",
    "        x2_norm = self.norm2(x2_relu)\n",
    "        x2_drop = F.dropout(x2_norm, p=self.drop, training=self.training)\n",
    "\n",
    "        x3 = self.conv3(x2_drop, edge_index, edge_weight=edge_weight)\n",
    "        x3_relu = self.leakyrelu3(x3)\n",
    "        x3_norm = self.norm3(x3_relu)\n",
    "        x3_drop = F.dropout(x3_norm, p=self.drop, training=self.training)\n",
    "\n",
    "        x4 = self.conv4(x3_drop, edge_index, edge_weight=edge_weight)\n",
    "        x4_relu = self.leakyrelu4(x4)\n",
    "        x4_norm = self.norm4(x4_relu)\n",
    "        x4_drop = F.dropout(x4_norm, p=self.drop, training=self.training)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x_emb = self.pooling(x4_drop, batch)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x_emb = F.dropout(x_emb, p=self.drop, training=self.training)\n",
    "        x_emb = self.lin1(x_emb)\n",
    "        x_emb = self.lin2(x_emb)\n",
    "        return x_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045892e",
   "metadata": {},
   "source": [
    "Training and Test Functions - Functions change if a 2-class or multiclass problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(classes) == 2:\n",
    "    def train(model, train_loader, optimizer):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        grad_norms = []\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            out = model(data.x.float(), data.edge_index, data.batch, data.edge_attr.float()) # Perform a forward pass.\n",
    "            \n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            losses.append(loss.to('cpu').detach().numpy())\n",
    "\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "        return np.mean(losses), grad_norms, model\n",
    "\n",
    "    def test(model, loader):\n",
    "        model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            out = model(data.x.float(), data.edge_index, data.batch, data.edge_attr.float())  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            losses.append(loss.to('cpu').detach().numpy())\n",
    "        return (correct / len(loader.dataset), np.mean(losses), out)  # Derive ratio of correct predictions.\n",
    "\n",
    "else:\n",
    "    def train(model, train_loader, optimizer):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        grad_norms = []\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            out = model(data.x.float(), data.edge_index, data.batch, data.edge_attr.float())# Perform a forward pass.\n",
    "            loss = criterion(out.cpu(), data.y.reshape(data.batch_size, len(classes)).type(\n",
    "                torch.FloatTensor)) # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            losses.append(loss.to('cpu').detach().numpy())\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "        return np.mean(losses), grad_norms, model\n",
    "\n",
    "    def test(model, loader):\n",
    "        model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            out = model(data.x.float(), data.edge_index, data.batch, data.edge_attr.float())  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct += int((pred.cpu() == data.y.reshape(data.batch_size, len(classes)).type(torch.FloatTensor).argmax(\n",
    "                dim=1)).sum())\n",
    "            loss = criterion(out.cpu(), data.y.reshape(data.batch_size, len(classes)).type(\n",
    "                torch.FloatTensor))  # Compute the loss.\n",
    "            losses.append(loss.to('cpu').detach().numpy())\n",
    "        return (correct / len(loader.dataset), np.mean(losses), out)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd476375",
   "metadata": {},
   "source": [
    "# Model Parameters\n",
    "\n",
    "The parameters chosen will be used for estimating model performance and to fit the model to extract metabolite importance.\n",
    "\n",
    "#### Model parameters to choose:\n",
    "\n",
    "- n_epochs - number of epochs to train the model\n",
    "- hidden_channels - number of hidden_channels/nodes in TAG layers\n",
    "- drop - dropout probability on dropout layers\n",
    "- K - number of hops in TAG layers\n",
    "- lr - learning rate of the model\n",
    "- weight decay - L2 Regularization\n",
    "- gamma - rate of exponential decay of the learning rate with epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb6070",
   "metadata": {},
   "source": [
    "## Estimating Model Performance\n",
    "\n",
    "Average of n iterations of different k-fold stratified cross validation. Both parameters were chosen at the beginning of the notebook.\n",
    "\n",
    "**This can take some time, put False to skip.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f055d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and training parameters\n",
    "n_epochs = 120 # 200\n",
    "hidden_channels = 64 # 128\n",
    "drop = 0.15 # 0.3\n",
    "K = 3 # 1\n",
    "lr = 0.005 # 0.001\n",
    "weight_decay = 0.0001 # 0\n",
    "gamma = 1 # 0.99\n",
    "batch_size= 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put True if you want ot see model performance, else put False\n",
    "see_model_performance = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80053349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed if wanted\n",
    "random_s = 174\n",
    "np.random.seed(random_s)\n",
    "\n",
    "if see_model_performance:\n",
    "    \n",
    "    # Setting parameters\n",
    "    max_epochs = n_epochs\n",
    "\n",
    "    # Setting up store results\n",
    "    loss_dict = {}\n",
    "    train_accuracy_dict = {}\n",
    "    test_accuracy_dict = {}\n",
    "    preds = 0\n",
    "    scores_dict = {n: 0 for n in range(max_epochs)}\n",
    "    accu_dict = {}\n",
    "\n",
    "    # For each repetition\n",
    "    for r in range(iter_num):\n",
    "        print('Iteration nº', r)\n",
    "        skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=random_s*(r+1))\n",
    "        loss_dict[r] = {}\n",
    "        train_accuracy_dict[r] = {}\n",
    "        test_accuracy_dict[r] = {}\n",
    "        correct = {n: 0 for n in range(max_epochs)}\n",
    "\n",
    "        # Each fold\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(treated_data, target)):\n",
    "            print('Fold nº', i)\n",
    "\n",
    "            train_loader = DataLoader(pd.Series(data_list_full)[train_index].values, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(pd.Series(data_list_full)[test_index].values, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Setting up the models\n",
    "            model = FDiGNN_TAG(hidden_channels=hidden_channels, drop=drop, n_node_feat=len(node_attrs),\n",
    "                               K=K, n_classes=len(classes)).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            # Temporary store lists\n",
    "            loss_list = []\n",
    "            train_accuracy_list = []\n",
    "            test_accuracy_list = []\n",
    "\n",
    "            # Train the model\n",
    "            for epoch in range(1, max_epochs+1):\n",
    "                model.float()\n",
    "                loss, g_norm, _ = train(model, train_loader, optimizer)\n",
    "                train_acc, _, _ = test(model, train_loader)\n",
    "                test_acc, _, _ = test(model, test_loader)\n",
    "                if epoch%10 == 0:\n",
    "                    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Learning Rate:{scheduler.get_last_lr()}')\n",
    "                loss_list.append(loss)\n",
    "                train_accuracy_list.append(train_acc)\n",
    "                test_accuracy_list.append(test_acc)\n",
    "                scheduler.step()\n",
    "\n",
    "            # Store results\n",
    "            loss_dict[r][i] = loss_list\n",
    "            train_accuracy_dict[r][i] = train_accuracy_list\n",
    "            test_accuracy_dict[r][i] = test_accuracy_list\n",
    "\n",
    "            preds += len(test_index)\n",
    "            for n in scores_dict:\n",
    "                correct[n] = correct[n] + test_accuracy_dict[r][i][n]*len(test_index)\n",
    "                scores_dict[n] = scores_dict[n] + test_accuracy_dict[r][i][n]*len(test_index)\n",
    "        \n",
    "        accu_dict[r] = pd.Series(correct)/len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d636d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "if see_model_performance:\n",
    "    print(f'Number of parameters: {sum(p.numel() for p in model.parameters())}.')\n",
    "    print('Max Epoch', (pd.Series(scores_dict)/preds).argmax()+1)\n",
    "    print('Max Acc.', (pd.Series(scores_dict)/preds).max())\n",
    "    print('-----')\n",
    "    print('Final Acc.', (pd.Series(scores_dict)/preds).iloc[-1], '+-', pd.DataFrame(accu_dict).std(axis=1).iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6528b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all accs\n",
    "#pd.Series(scores_dict)/preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b057b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all accs std\n",
    "#pd.DataFrame(accu_dict).std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219431e",
   "metadata": {},
   "source": [
    "## Fitting the Model with All Samples\n",
    "\n",
    "Fit the model with all samples. From the model with all samples, we can extract metabolite importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(174)\n",
    "\n",
    "# Setting parameters\n",
    "max_epochs = n_epochs\n",
    "\n",
    "classes = pd.unique(target)\n",
    "\n",
    "print('Starting model fitting.')\n",
    "\n",
    "# Train the model\n",
    "train_loader = DataLoader(pd.Series(data_list_full).values, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(pd.Series(data_list_full).values, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Setting up the models\n",
    "model = FDiGNN_TAG(hidden_channels=hidden_channels, drop=drop, n_node_feat=len(node_attrs), K=K,\n",
    "                   n_classes=len(classes)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Model Training\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    model.float()\n",
    "    loss, g_norm, _ = train(model, train_loader, optimizer)\n",
    "    train_acc, _, _ = test(model, train_loader)\n",
    "    test_acc, _, _ = test(model, test_loader)\n",
    "    if epoch%10 == 0:\n",
    "        print(f'''Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f},\n",
    "              Learning Rate:{scheduler.get_last_lr()}''')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of archiecture and parameters of models fit\n",
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce0559b",
   "metadata": {},
   "source": [
    "## Metabolite / Node Importance\n",
    "\n",
    "Metabolite Importance is estimated by seeing the imapct changes in the features of the corresponding node have on the prediction probability of the different samples. Broadly speaking, the metabolite importance calculation methods is as follow:\n",
    "\n",
    "1) Get the model predictions probabilities of each class for every sample unchanged.\n",
    "\n",
    "2) For each node, change its intensity and feature occurrence to low, intermediate and high values in all samples. These values are defined by the quantile values of that node/metabolite across all samples. Default quantile values are 0.05, 0.50 and 0.95.\n",
    "\n",
    "3) Get the model predictions probabilities for the changed samples with each of the quantiles. Then, for each sample, calculate the absolute prediction probability change in comparison to the unchaged sample for the chosen quantiles. The highest absolute change is stored.\n",
    "\n",
    "4) We then obtain for each sample and each node, how their class predictions change based on the changing of the node features. Since samples can be easier or more difficult to change predictions, a normalization is performed by dividing the values by the sum of the predictions changes associated to each node for a sample. Thus, all samples contribute the same for the calculation of metabolite importance.\n",
    "\n",
    "5) Finally, for each node, we calculate the median of all samples normalized prediction changes to get a score of the importance of that node/metabolite.\n",
    "\n",
    "**Note: After this, we see the top 20 most important metabolites. Observe the score values of the first few metabolites to see if the top 1 is not orders of magnitude higher than the others. If it is, overfitting may have occurred and model parameters might need to be changed. If they are not, then the model is focusing on multiple nodes which is what we want.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store eery prediction made - can occupy space\n",
    "store_all_preds = False\n",
    "\n",
    "effect = {} # Store the prediction impact\n",
    "all_preds ={} # Store all predictions\n",
    "\n",
    "# Normal Predictions in unchanged samples\n",
    "out_normal = pd.DataFrame()\n",
    "for data in DataLoader(data_list_full, batch_size=32, shuffle=False):  # Iterate in batches over the training/test dataset.\n",
    "    model.eval()\n",
    "    out = model(data.x.float(), data.edge_index, data.batch, data.edge_attr.float())\n",
    "    out = F.softmax(out, 1)\n",
    "    out_normal = pd.concat((out_normal, pd.DataFrame(out.detach().cpu().numpy())))\n",
    "\n",
    "if store_all_preds:\n",
    "    all_preds['Normal'] = out_normal.reset_index().iloc[:, 1:].to_dict()\n",
    "\n",
    "# For each metabolite/node\n",
    "for i in tqdm(range(len(FDiN.nodes()))):\n",
    "    # Get node index, unchanged values and see the quantile values\n",
    "    node = list(FDiN.nodes())[i]\n",
    "    original_values = treated_data.loc[:, node].copy().values\n",
    "    original_feature_values = bin_data.loc[:, node].copy().values\n",
    "    q_values = [0.05, 0.5, 0.95]\n",
    "\n",
    "    # Set the stores\n",
    "    if store_all_preds:\n",
    "        all_preds[node] = {}\n",
    "    effect[i] = pd.DataFrame(columns=q_values)\n",
    "    # Get the quantiles\n",
    "    quantile_values = np.quantile(original_values, q=q_values)\n",
    "    quantile_feature_values = np.quantile(original_feature_values, q=q_values)\n",
    "    \n",
    "    for q in range(len(quantile_values)):\n",
    "        # Change all samples for the current quantile value\n",
    "        for g in range(len(data_list_full)):\n",
    "            data_list_full[g].x[i, -2] = quantile_values[q]\n",
    "            if quantile_feature_values[q] != 0:\n",
    "                if quantile_feature_values[q] != 1:\n",
    "                    data_list_full[g].x[i, -1] = 1\n",
    "                else:\n",
    "                    data_list_full[g].x[i, -1] = quantile_feature_values[q]\n",
    "            else:\n",
    "                data_list_full[g].x[i, -1] = quantile_feature_values[q]\n",
    "\n",
    "        # See and store model predictions of changed samples\n",
    "        out_shuffled = pd.DataFrame()\n",
    "        for data in DataLoader(data_list_full, batch_size=32, shuffle=False):  # Iterate in batches over the training/test dataset.\n",
    "            model.eval()\n",
    "            out = model(data.x.float(), data.edge_index, data.batch, data.edge_attr.float())\n",
    "            out = F.softmax(out, 1)\n",
    "            out_shuffled = pd.concat((out_shuffled, pd.DataFrame(out.detach().cpu().numpy())))\n",
    "        if store_all_preds:\n",
    "            all_preds[node][q] = out_shuffled.reset_index().iloc[:, 1:].to_dict()\n",
    "\n",
    "        # Calculate the absolute prediction differences and store them\n",
    "        results = pd.DataFrame((out_normal.values - out_shuffled.values)).abs()\n",
    "        effect[i][q_values[q]] = results.max(axis=1)\n",
    "\n",
    "    # Restore values\n",
    "    for g in range(len(data_list_full)):\n",
    "        data_list_full[g].x[i, -2] = original_values[g]\n",
    "        data_list_full[g].x[i, -1] = original_feature_values[g]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868631dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the results from the different nodes, quantiles and samples\n",
    "new_df = pd.DataFrame(columns=range(len(sample_cols)))\n",
    "for node in effect.keys():\n",
    "    # Get the maximum values for each quantile for each node/sample pair\n",
    "    new_df.loc[node] = effect[node].max(axis=1).values\n",
    "new_df.index = list(FDiN.nodes())\n",
    "\n",
    "# Normalize all predictions changes by sample - make each sample have the same weight for node importance calculation\n",
    "new_df = (new_df/new_df.sum()).replace({np.nan:0})\n",
    "global_effect = new_df.T.apply(\n",
    "    lambda x: x.sort_values(ascending=False).values).T\n",
    "\n",
    "# Get the median of all normalzied prediction changes across the samples for the final importance score\n",
    "global_effect = global_effect.median(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd179ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the top 20 nodes and how much change\n",
    "global_effect.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2b7c2",
   "metadata": {},
   "source": [
    "# Analysing FDiGNN Important Metabolites\n",
    "\n",
    "Select the top % of metabolitees that will be considered as important. This can change based on dataset size, the higher the dataset, the smaller the threshold so results can be interpretable.\n",
    "\n",
    "Furthermore, select the minimum requirements for a pathway to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base selection of top metabolites to see (in % - 0.1 is the top 10% of metabolites)\n",
    "base_threshold_of_importance = 0.10\n",
    "\n",
    "# Choose the minimum number of metabolites detected associated to a pathway to consider that pathway\n",
    "n_min_nodes_path = 3\n",
    "# Choose the minimum number of eedges between detected metabolites associated to a pathway to consider that pathway\n",
    "n_min_edges_path = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking the importances\n",
    "all_ranks=pd.DataFrame(global_effect).rank(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed84b75",
   "metadata": {},
   "source": [
    "## Pathway Enrichment Analysis\n",
    "\n",
    "- Background Set - Number of Metabolites detected in the FDiN with at least one associated pathway\n",
    "- Pathway Background Set - Number of Metabolites detected in the FDiN in each pathway\n",
    "- 'Significant' Metabolites - Number of Metabolites in the top ranks considered with at least one associated pathway\n",
    "- 'Significant' Metabolites in Pathway - Number of Metabolites in the top ranks considered in each pathway\n",
    "\n",
    "'Signigicant' Metabolites are thos that are within the top % of FDiGNN importance ranks with the % defined by the parameter `base_threshold_of_importance` (e.g. 0.10 is the top 10%) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c85636",
   "metadata": {},
   "source": [
    "Very similar pathways repeated many times were merged to a single pathway:\n",
    "\n",
    "- 22657 pathway names starting with 'De Novo Triacylglycerol Biosynthesis' merged to 'De Novo Triacylglycerol Biosynthesis'\n",
    "- 923 pathway names starting with 'Phosphatidylethanolamine Biosynthesis' merged to 'Phosphatidylethanolamine Biosynthesis'\n",
    "- 923 pathway names starting with 'Phosphatidylcholine Biosynthesis' merged to 'Phosphatidylcholine Biosynthesis'\n",
    "- 3278 pathway names starting with 'Cardiolipin Biosynthesis' merged to 'Cardiolipin Biosynthesis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a907417",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in FDiN_knowledge.nodes():\n",
    "    n_p = []\n",
    "    for p in FDiN_knowledge.nodes()[node]['Pathways']:\n",
    "        if p.startswith('De Novo Triacylglycerol Biosynthesis'):\n",
    "            if 'De Novo Triacylglycerol Biosynthesis' not in n_p:\n",
    "                n_p.append('De Novo Triacylglycerol Biosynthesis')\n",
    "        elif p.startswith('Phosphatidylethanolamine Biosynthesis'):\n",
    "            if 'Phosphatidylethanolamine Biosynthesis' not in n_p:\n",
    "                n_p.append('Phosphatidylethanolamine Biosynthesis')\n",
    "        elif p.startswith('Phosphatidylcholine Biosynthesis'):\n",
    "            if 'Phosphatidylcholine Biosynthesis' not in n_p:\n",
    "                n_p.append('Phosphatidylcholine Biosynthesis')\n",
    "        elif p.startswith('Cardiolipin Biosynthesis'):\n",
    "            if 'Cardiolipin Biosynthesis' not in n_p:\n",
    "                n_p.append('Cardiolipin Biosynthesis')\n",
    "        else:\n",
    "            n_p.append(p)\n",
    "    FDiN_knowledge.nodes()[node]['Pathways'] = n_p\n",
    "\n",
    "for edge in FDiN_knowledge.edges():\n",
    "    n_p = []\n",
    "    for p in FDiN_knowledge.edges()[edge]['Pathways']:\n",
    "        if p.startswith('De Novo Triacylglycerol Biosynthesis'):\n",
    "            if 'De Novo Triacylglycerol Biosynthesis' not in n_p:\n",
    "                n_p.append('De Novo Triacylglycerol Biosynthesis')\n",
    "        elif p.startswith('Phosphatidylethanolamine Biosynthesis'):\n",
    "            if 'Phosphatidylethanolamine Biosynthesis' not in n_p:\n",
    "                n_p.append('Phosphatidylethanolamine Biosynthesis')\n",
    "        elif p.startswith('Phosphatidylcholine Biosynthesis'):\n",
    "            if 'Phosphatidylcholine Biosynthesis' not in n_p:\n",
    "                n_p.append('Phosphatidylcholine Biosynthesis')\n",
    "        elif p.startswith('Cardiolipin Biosynthesis'):\n",
    "            if 'Cardiolipin Biosynthesis' not in n_p:\n",
    "                n_p.append('Cardiolipin Biosynthesis')\n",
    "        else:\n",
    "            n_p.append(p)\n",
    "    FDiN_knowledge.edges()[edge]['Pathways'] = n_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa5f10",
   "metadata": {},
   "source": [
    "Use the metabolic knowledge network to associate metabolic pathways to nodes (if the formula is in the pathway) and to edges (if both connecting formulas are in the pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6750cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Nodes\n",
    "# Formulas in the metabolic knowledge network and in the FDiN\n",
    "formulas_in_knowledge = list(FDiN_knowledge.nodes())\n",
    "# Set up stores\n",
    "pathway_ranks = {} # Stores only the ranks of the nodes related to the pathway\n",
    "pathway_series = {} # Stores the nodes related to the pathway and their ranks\n",
    "\n",
    "# For each node\n",
    "for node in FDiN.nodes():\n",
    "    curr_formula = FDiN.nodes()[node]['Formula']\n",
    "    # See the annotation to add as an attribute\n",
    "    if isinstance(processed_data.loc[node, 'Matched names'], list):\n",
    "        FDiN.nodes()[node]['Name'] = processed_data.loc[node, 'Matched names']\n",
    "        FDiN.nodes()[node]['HMDB_ID'] = processed_data.loc[node, 'Matched IDs']\n",
    "    else:\n",
    "        FDiN.nodes()[node]['Name'] = 'None'\n",
    "        FDiN.nodes()[node]['HMDB_ID'] = 'None'\n",
    "\n",
    "    # If the formula is in the metabolic network, add the Pathway related information\n",
    "    if curr_formula in formulas_in_knowledge:\n",
    "        FDiN.nodes()[node]['Names in Pathways'] = FDiN_knowledge.nodes()[curr_formula]['Names']\n",
    "        FDiN.nodes()[node]['HMDB_ID in Pathways'] = FDiN_knowledge.nodes()[curr_formula]['HMDB_ID']\n",
    "        FDiN.nodes()[node]['Pathways'] = FDiN_knowledge.nodes()[curr_formula]['Pathways']\n",
    "        FDiN.nodes()[node]['SMPDB_IDs'] = FDiN_knowledge.nodes()[curr_formula]['SMPDB_IDs']\n",
    "\n",
    "        # And add the FDiGNN metabolite importance rank to the pathway stores\n",
    "        for p in FDiN_knowledge.nodes()[curr_formula]['Pathways']:\n",
    "            if p in pathway_ranks:\n",
    "                pathway_ranks[p].append(all_ranks.loc[node, 0])\n",
    "                pathway_series[p].loc[node] = all_ranks.loc[node, 0]\n",
    "            else:\n",
    "                pathway_ranks[p] = [all_ranks.loc[node, 0],]\n",
    "                pathway_series[p] = pd.Series()\n",
    "                pathway_series[p].loc[node] = all_ranks.loc[node, 0]\n",
    "\n",
    "    # If not associated to a metabolic pathway\n",
    "    else:\n",
    "        FDiN.nodes()[node]['Names in Pathways'] = 'None'\n",
    "        FDiN.nodes()[node]['HMDB_ID in Pathways'] = 'None'\n",
    "        FDiN.nodes()[node]['Pathways'] = 'None'\n",
    "        FDiN.nodes()[node]['SMPDB_IDs'] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Edges - Adding pathway information\n",
    "for edge in FDiN.edges():\n",
    "    formA = FDiN.nodes()[edge[0]]['Formula']\n",
    "    formB = FDiN.nodes()[edge[1]]['Formula']\n",
    "    if (formA, formB) in FDiN_knowledge.edges():\n",
    "        FDiN.edges()[edge]['Pathways'] = FDiN_knowledge.edges()[(formA, formB)]['Pathways']\n",
    "        FDiN.edges()[edge]['SMPDB_IDs'] = FDiN_knowledge.edges()[(formA, formB)]['SMPDB_IDs']\n",
    "    else:\n",
    "        FDiN.edges()[edge]['Pathways'] = 'None'\n",
    "        FDiN.edges()[edge]['SMPDB_IDs'] = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873b821",
   "metadata": {},
   "source": [
    "Re-rank pathway associated nodes to skip the non-pathway associated nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the nodes with associated pathways and their ranks\n",
    "# Then make a dict to re-order those ranks skipping nodes without associated pathways\n",
    "s = pd.DataFrame(pathway_series).values.flatten()\n",
    "s = pd.unique(pd.Series(s[~np.isnan(s)]).sort_values())\n",
    "path_ranks_short = dict(zip(s,range(1, len(s)+1)))\n",
    "\n",
    "# Use the dictionary made to get the ranks per metabolic pathway after this association\n",
    "for i in pathway_ranks:\n",
    "    pathway_ranks[i].sort()\n",
    "for p in pathway_ranks:\n",
    "    pathway_ranks[p] = [path_ranks_short[i] for i in pathway_ranks[p]]\n",
    "#pathway_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f52df",
   "metadata": {},
   "source": [
    "Limit the pathways considered based on the parameters defined before (number of nodes and edges associated to that pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the pathways considered based on the minimum number of nodes associated with it\n",
    "paths = pd.Series(nx.get_node_attributes(FDiN, 'Pathways')).explode().value_counts()\n",
    "paths.pop('None')\n",
    "paths = paths[paths>=n_min_nodes_path]\n",
    "\n",
    "# Limit the pathways considered based on the minimum number of edges nodes associated with it have\n",
    "paths_edges = pd.Series(nx.get_edge_attributes(FDiN, 'Pathways')).explode().value_counts()\n",
    "paths_edges.pop('None')\n",
    "paths_edges = paths_edges[paths_edges>=n_min_edges_path]\n",
    "\n",
    "# Combine the two filters to obtain the total pathways considered\n",
    "paths = paths[[p for p in paths.index if p in paths_edges.index]]\n",
    "paths_edges = paths_edges[paths.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22ec95",
   "metadata": {},
   "source": [
    "#### Pathway Enrichment Analysis\n",
    "\n",
    "- Background Set - Number of Metabolites detected in the FDiN with at least one associated pathway\n",
    "- Pathway Background Set - Number of Metabolites detected in the FDiN in each pathway\n",
    "- 'Significant' Metabolites - Number of Metabolites in the top ranks considered with at least one associated pathway\n",
    "- 'Significant' Metabolites in Pathway - Number of Metabolites in the top ranks considered in each pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Significant' Metabolites - Number of Metabolites in the top ranks considered with at least one associated pathway\n",
    "top_ranks_considered = (\n",
    "    np.array(list(path_ranks_short.keys())) <= int(base_threshold_of_importance * len(FDiN.nodes()))).sum()\n",
    "annotated_metabolites_w_path = top_ranks_considered\n",
    "\n",
    "# Background Set - Number of Metabolites detected in the FDiN with at least one associated pathway\n",
    "total_metabolites_with_pathways = len(path_ranks_short)\n",
    "\n",
    "# Get the counts of metabolites of each pathway\n",
    "path_assign_hmdb = paths\n",
    "\n",
    "# Preparing DF\n",
    "over_representation_df = pd.DataFrame(columns=[\n",
    "    'Nº of Met. in Dataset', 'Nº of Met. in Pathway', '% of Met. In Set', 'Probability'], dtype='object')\n",
    "\n",
    "for pathway in tqdm(path_assign_hmdb.index):\n",
    "    # Pathway Name\n",
    "    p_name = pathway\n",
    "    # Pathway Background Set - Number of Metabolites detected in the FDiN in each pathway\n",
    "    total_met_in_path = path_assign_hmdb.loc[pathway]\n",
    "    # 'Significant' Metabolites in Pathway - Number of Metabolites in the top ranks considered in each pathway\n",
    "    ann_met_in_path = (np.array(pathway_ranks[pathway]) <= top_ranks_considered).sum()\n",
    "\n",
    "    # Calculating probability to find ann_met_in_path or more metabolites in annotated_metabolites_w_path\n",
    "    prob = stats.hypergeom(M=total_metabolites_with_pathways, \n",
    "                            n=total_met_in_path, \n",
    "                            N=annotated_metabolites_w_path).sf(ann_met_in_path-1)\n",
    "\n",
    "    # Adding the line to the DF\n",
    "    over_representation_df.loc[pathway] = [ann_met_in_path, total_met_in_path,\n",
    "                                           ann_met_in_path/total_met_in_path*100, prob]\n",
    "\n",
    "# Sorting DataFrame from least probable to more probable and adding adjusted probability\n",
    "# with Benjamini-Hochberg multiple test correction\n",
    "over_representation_df_nodes = over_representation_df.sort_values(by='Probability')\n",
    "over_representation_df_nodes['Adjusted (BH) Probability'] = metsta.p_adjust_bh(over_representation_df_nodes['Probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b474d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from the Over-Representation Analysis\n",
    "over_representation_df_nodes.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d115f",
   "metadata": {},
   "source": [
    "## Dash App For Highlighting Important Network Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of sorted importances to use to map\n",
    "sorted_imps = (global_effect).rank(ascending=False).sort_values()\n",
    "sorted_imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Extra layouts for saving svg files\n",
    "cyto.load_extra_layouts()\n",
    "\n",
    "# Transform FDiN into Cytoscape data\n",
    "cs_data = nx.cytoscape_data(FDiN)\n",
    "elements = cs_data[\"elements\"][\"nodes\"] + cs_data[\"elements\"][\"edges\"]\n",
    "\n",
    "# Make a 'graph' to act as node legend\n",
    "graph_legend = nx.Graph()\n",
    "graph_legend.add_nodes_from(['Pathway Node','Rel. Near Pathway','Non-Pathway Node'])\n",
    "nx.set_node_attributes(graph_legend, {'Pathway Node':{'color':'Red'},\n",
    "                                       'Rel. Near Pathway':{'color':'lightcoral'},\n",
    "                                       'Non-Pathway Node':{'color':'lightgrey'}})\n",
    "cs_legend = nx.cytoscape_data(graph_legend)\n",
    "for n, p in zip(cs_legend[\"elements\"][\"nodes\"], range(0,250,50)):\n",
    "    n[\"position\"] = {\"x\": -200, \"y\": p}\n",
    "elements_legend = cs_legend[\"elements\"][\"nodes\"] + cs_legend[\"elements\"][\"edges\"]\n",
    "\n",
    "# Get an initial order of pathways\n",
    "rel_paths = over_representation_df_nodes.index\n",
    "\n",
    "# Get specific positions for preset layout of the network\n",
    "pos = nx.kamada_kawai_layout(FDiN)\n",
    "for n1, p in zip(cs_data[\"elements\"][\"nodes\"], pos.values()):\n",
    "    n1[\"position\"] = {\"x\": p[0] * 1700, \"y\": p[1] * 1700}\n",
    "\n",
    "# Initiate the App\n",
    "app = Dash()\n",
    "\n",
    "# Dropdown menu for network layout\n",
    "dropdown_layout = dcc.Dropdown(\n",
    "    id='dropdown-update-layout',\n",
    "    value='preset',\n",
    "    clearable=False,\n",
    "    options=[\n",
    "        {'label': name.capitalize(), 'value': name}\n",
    "        for name in ['preset', 'grid', 'random', 'circle', 'cose', 'concentric']\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Dropdown menu for pathway chosen\n",
    "dropdown_layout2 = dcc.Dropdown(\n",
    "    id='dropdown-update-layout2',\n",
    "    value='None',\n",
    "    clearable=False,\n",
    "    options=[\n",
    "        {'label': name.capitalize(), 'value': name}\n",
    "        for name in ['None',] + list(rel_paths)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Slider to select the threshold to use to consider important metabolites\n",
    "top_mets_chosen = dcc.Slider(0, 0.5, 0.005, value=0.05, id='top_mets_chosen',\n",
    "                            marks={0: {'label': '0'}, 0.02: {'label': '0.02'}, 0.05: {'label': '0.05'}, \n",
    "                                   0.1: {'label': '0.10'}, 0.2: {'label': '0.20'}, 0.5: {'label': '0.50'} })\n",
    "\n",
    "# Organize the APP layout\n",
    "app.layout = html.Div([html.Div([# Initial Options\n",
    "                       dcc.Markdown('Graph Layout:',),\n",
    "                       dropdown_layout,\n",
    "                       dcc.Markdown('Pathway Highlighted:',),\n",
    "                       dropdown_layout2,\n",
    "                       dcc.Markdown('Nº of Important Metabolites:',),\n",
    "                       top_mets_chosen,\n",
    "    # Put the Network\n",
    "    cyto.Cytoscape(\n",
    "        id='cytoscape',\n",
    "        elements=elements,\n",
    "        style={'width': '100%', 'height': '800px'},\n",
    "        layout={\n",
    "            'name': 'preset'\n",
    "        },\n",
    "         stylesheet=[\n",
    "                    {'selector': 'node',\n",
    "                'style': {'label': 'data(cname)',\n",
    "                    'background-color': 'data(color)',\n",
    "                    'shape': 'circle'}},\n",
    "                     {'selector': 'edge',\n",
    "                'style': {\n",
    "                    'line-color': 'data(color)'}}]\n",
    "    )], style={'flex': 2}),\n",
    "                    html.Div([# Right part of App - Information\n",
    "        # Info when clicking a node\n",
    "        html.P(id='cytoscape-tapNodeData-output'), # General Info\n",
    "        html.Img(id='bar-graph-matplotlib'), # Bar Plot\n",
    "        html.P(id='node-presence-output'), # Feature Occurrence\n",
    "        # Information when clicking an edge\n",
    "        html.P(id='cytoscape-tapEdgeData-output'),\n",
    "        # Information on the highlighted network section\n",
    "        dcc.Markdown('### Information',),\n",
    "        html.P(id='cytoscape-selected-area-output'),\n",
    "        # Buttons to save figure\n",
    "        html.Div('Download graph:'),\n",
    "        html.Button(\"as jpg\", id=\"btn-get-jpg\"),\n",
    "        html.Button(\"as png\", id=\"btn-get-png\"),\n",
    "        html.Button(\"as svg\", id=\"btn-get-svg\")], style={'flex': 1, 'padding': 10})], \n",
    "                      \n",
    "    style={'display': 'flex', 'flexDirection': 'row'}\n",
    ")\n",
    "\n",
    "\n",
    "## Fucntions to respond to changes in the app\n",
    "\n",
    "@callback(Output('cytoscape', 'stylesheet'),\n",
    "          Output('cytoscape', 'elements'),\n",
    "          Output('cytoscape-selected-area-output', 'children'),\n",
    "          Output(component_id='dropdown-update-layout2', component_property='options'),\n",
    "              Input('top_mets_chosen', 'value'),\n",
    "              Input('dropdown-update-layout2', 'value'))\n",
    "def update_layout(top_chosen, pathway):\n",
    "    \"Updates layout based on the threshold and pathway chosen\"\n",
    "\n",
    "    ### Top chosen metabolites\n",
    "    top_chosen = int(top_chosen*len(FDiN.nodes()))\n",
    "    imp_mzs = list(sorted_imps.index[:top_chosen])\n",
    "\n",
    "    ### Pathway enrichment Analysis as before\n",
    "    top_ranks_considered = (np.array(list(path_ranks_short.keys())) <= top_chosen).sum()\n",
    "\n",
    "    # Background Set - Number of Metabolites detected in the FDiN with at least one associated pathway\n",
    "    total_metabolites_with_pathways = len(path_ranks_short)\n",
    "\n",
    "    # Get the counts of metabolites of each pathway\n",
    "    path_assign_hmdb = paths\n",
    "\n",
    "    # 'Significant' Metabolites - Number of Metabolites in the top ranks considered with at least one associated pathway\n",
    "    annotated_metabolites_w_path = top_ranks_considered\n",
    "\n",
    "    # Preparing DF\n",
    "    over_representation_df = pd.DataFrame(columns=['Pathway Name',\n",
    "        'Nº of Met. in Dataset', 'Nº of Met. in Pathway', '% of Met. In Set', 'Probability'], dtype='object')\n",
    "\n",
    "    for pathway2 in tqdm(path_assign_hmdb.index):\n",
    "        # Pathway Name\n",
    "        p_name = pathway2\n",
    "        # Pathway Background Set - Number of Metabolites detected in the FDiN in each pathway\n",
    "        total_met_in_path = path_assign_hmdb.loc[pathway2]\n",
    "        # 'Significant' Metabolites in Pathway - Number of Metabolites in the top ranks considered in each pathway\n",
    "        ann_met_in_path = (np.array(pathway_ranks[pathway2]) <= top_ranks_considered).sum()\n",
    "\n",
    "        # Calculating probability to find ann_met_in_path or more metabolites in annotated_metabolites_w_path\n",
    "        prob = stats.hypergeom(M=total_metabolites_with_pathways, \n",
    "                                n=total_met_in_path, \n",
    "                                N=annotated_metabolites_w_path).sf(ann_met_in_path-1)\n",
    "\n",
    "        # Adding the line to the DF\n",
    "        over_representation_df.loc[pathway2] = [p_name, ann_met_in_path, total_met_in_path,\n",
    "                                               ann_met_in_path/total_met_in_path*100, prob]\n",
    "\n",
    "    # Sorting DataFrame from least probable to more probable and adding adjusted probability\n",
    "    # with Benjamini-Hochberg multiple test correction\n",
    "    over_representation_df_nodes = over_representation_df.sort_values(by='Probability')\n",
    "\n",
    "\n",
    "    ## Get color, node names to show and opacity of nodes for each\n",
    "    colors_nodes = {}\n",
    "    cname_nodes = {}\n",
    "    opacity_nodes = {}\n",
    "    for i in FDiN.nodes():\n",
    "        if i in imp_mzs:\n",
    "            if pathway in FDiN.nodes()[i]['Pathways'] and pathway != 'None':\n",
    "                colors_nodes[i] ='red'\n",
    "            else:\n",
    "                colors_nodes[i] ='lightcoral'\n",
    "            cname_nodes[i] = temp_df.loc[i ,'Formula_Assignment']\n",
    "            opacity_nodes[i] = 1\n",
    "        else:\n",
    "            if pathway in FDiN.nodes()[i]['Pathways'] and pathway != 'None':\n",
    "                colors_nodes[i] ='grey'\n",
    "                opacity_nodes[i] = 1\n",
    "            else:\n",
    "                colors_nodes[i] = 'lightgrey'\n",
    "                opacity_nodes[i] = 0.4\n",
    "            cname_nodes[i] =''\n",
    "    for n, c in zip(cs_data[\"elements\"][\"nodes\"], colors_nodes.values()):\n",
    "        n['data'][\"color\"] = c\n",
    "    for n, c in zip(cs_data[\"elements\"][\"nodes\"], cname_nodes.values()):\n",
    "        n['data'][\"cname\"] = c\n",
    "    for n, c in zip(cs_data[\"elements\"][\"nodes\"], opacity_nodes.values()):\n",
    "        n['data'][\"opacity\"] = c\n",
    "\n",
    "    ## Get color and opacity of edges\n",
    "    edge_colors = {}\n",
    "    edge_opacity = {}\n",
    "    for i in FDiN.edges():\n",
    "        if i[0] in imp_mzs and i[1] in imp_mzs:\n",
    "            if pathway in FDiN.edges()[i]['Pathways'] and pathway != 'None':\n",
    "                edge_colors[i] ='red'\n",
    "            else:\n",
    "                edge_colors[i] ='lightcoral'\n",
    "            edge_opacity[i] = 1\n",
    "        else:\n",
    "            if pathway in FDiN.edges()[i]['Pathways'] and pathway != 'None':\n",
    "                edge_colors[i] ='grey'\n",
    "                edge_opacity[i] = 1\n",
    "            else:\n",
    "                edge_colors[i] = 'lightgrey'\n",
    "                edge_opacity[i] = 0.4\n",
    "    for n, c in zip(cs_data[\"elements\"][\"edges\"], edge_colors.values()):\n",
    "        n['data'][\"color\"] = c\n",
    "    for n, c in zip(cs_data[\"elements\"][\"edges\"], edge_opacity.values()):\n",
    "        n['data'][\"opacity\"] = c\n",
    "\n",
    "    # Build the string to describe network component of highlighted area\n",
    "    construct_string = [html.B(f'Components of top {top_chosen} important metabolites:'), html.Br(), html.Br(),]\n",
    "    for i in nx.connected_components(FDiN.subgraph(imp_mzs)):\n",
    "        construct_string.append(html.B(f'{len(i)}-length component'))\n",
    "        construct_string.append(f'{i}).')\n",
    "        construct_string.append(html.Br())\n",
    "\n",
    "    # Return in order to update layout\n",
    "    return [{'selector': 'node',\n",
    "                'style': {'label': 'data(cname)',\n",
    "                    'background-color': 'data(color)',\n",
    "                    'shape': 'circle',\n",
    "                    'opacity': 'data(opacity)'}},\n",
    "            {'selector': 'edge',\n",
    "                'style': {\n",
    "                'line-color': 'data(color)',\n",
    "                'opacity': 'data(opacity)'}}], cs_data[\"elements\"][\"nodes\"] + cs_data[\"elements\"][\n",
    "        \"edges\"], construct_string, [{'label': name.capitalize(), 'value': name}\n",
    "                                for name in ['None',] + list(over_representation_df_nodes.index)]\n",
    "\n",
    "@callback(Output('cytoscape', 'layout'),\n",
    "              Input('dropdown-update-layout', 'value'))\n",
    "def update_layout(layout):\n",
    "    return {'name': layout, 'animate': False}\n",
    "\n",
    "@callback(Output('cytoscape-tapNodeData-output', 'children'),\n",
    "          Output(component_id='bar-graph-matplotlib', component_property='src'),\n",
    "          Output(component_id='node-presence-output', component_property='children'),\n",
    "              Input('cytoscape', 'tapNodeData'))\n",
    "def displayTapNodeData(data):\n",
    "    \"Display information on the node, its average intensities and feature occurrence across classes.\"\n",
    "    if data:\n",
    "        \n",
    "        ## Node Information Section\n",
    "        construct_string = [html.B(f'Most recently clicked node: {data[\"id\"]}'), html.Br(), html.Br(),]\n",
    "        for cat in ['id', 'Formula', 'Name', 'HMDB_ID', 'Names in Pathways',\n",
    "                    'HMDB_ID in Pathways', 'Pathways', 'SMPDB_IDs']:\n",
    "            if cat == 'Name':\n",
    "                # Include VIP score Rank\n",
    "                construct_string.extend([f'FDiGNN Rank: {str(all_ranks.loc[data[\"id\"]].iloc[0])}',  html.Br(),])\n",
    "            construct_string.extend([f'{cat.capitalize()}: {str(data[cat])}',  html.Br(),])\n",
    "\n",
    "        ## Normalized Intensity Bar Plot Section\n",
    "        plt.close()\n",
    "        gfinder = processed_data.copy().loc[data[\"id\"], sample_cols]\n",
    "        percentages = {}\n",
    "        # Calculate average intensities (and std)\n",
    "        for cl in classes:\n",
    "            section = gfinder.iloc[[i for i in range(len(sample_cols)) if target[i]==cl]]#.mean()\n",
    "            percentages[cl] = section.notnull().sum()/len(section)*100\n",
    "            gfinder[cl+' Average'] = section.mean()\n",
    "            gfinder[cl+' sem'] = section.std() / np.sqrt(section.notnull().sum())\n",
    "        avg_cols = [col for col in gfinder.index if 'Average' in col]\n",
    "        sem_cols = [col for col in gfinder.index if 'sem' in col]\n",
    "        # Plot the graph\n",
    "        bar_plot_info = gfinder.replace({np.nan:0})\n",
    "        factor = 1\n",
    "        maxi_v = (np.array(bar_plot_info.loc[avg_cols])*10**factor).max()\n",
    "        while maxi_v < 1:\n",
    "            factor+=1\n",
    "            maxi_v = (np.array(bar_plot_info.loc[avg_cols])*10**factor).max()\n",
    "        fig, ax = plt.subplots(1,1, figsize=(5,3), constrained_layout=True)\n",
    "        x = np.arange(len(avg_cols))\n",
    "        if len(classes) <= 10:\n",
    "            colors_for_plot = [colours[i] for i in range(len(classes))]\n",
    "        else:\n",
    "            colors_for_plot = [colours[i] for i in range(10)]\n",
    "        ax.bar(x, np.array(bar_plot_info.loc[avg_cols])*10**factor, color=colors_for_plot,\n",
    "               yerr=np.array(bar_plot_info.loc[sem_cols])*10**factor, capsize=12)\n",
    "        ax.set_ylabel(f'Intensity (x$10^{factor}$)', fontsize=15)\n",
    "        ax.set_title(processed_data.copy().loc[data[\"id\"], 'Formula_Assignment'], fontsize=15)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(classes, fontsize=12)\n",
    "        # Embed the result in the html output.\n",
    "        buf = BytesIO()\n",
    "        fig.savefig(buf, format=\"png\")\n",
    "        fig_data = base64.b64encode(buf.getbuffer()).decode(\"ascii\")\n",
    "        fig_bar_matplotlib = f'data:image/png;base64,{fig_data}'\n",
    "\n",
    "        ## Feature Occurrence Section\n",
    "        node_pres = [html.B(f'Node Present in % of Class Samples:'), html.Br(), html.Br(),]\n",
    "        for cl in percentages:\n",
    "            node_pres.extend([f'{cl}: {percentages[cl]:.3f}', html.Br()])\n",
    "        \n",
    "        return construct_string, fig_bar_matplotlib, node_pres\n",
    "    return '', '', ''\n",
    "\n",
    "@callback(Output('cytoscape-tapEdgeData-output', 'children'),\n",
    "              Input('cytoscape', 'tapEdgeData'))\n",
    "def displayTapEdgeData(data):\n",
    "    \"Show the edge data, that is if it can be mapped to a pathway.\"\n",
    "    if data:\n",
    "        construct_string = [html.B(f'Most recently clicked edge: {data[\"source\"]}-{data[\"target\"]}'),\n",
    "                            html.Br(), html.Br(),]\n",
    "        for cat in ['Pathways', 'SMPDB_IDs']:\n",
    "            construct_string.extend([f'{cat.capitalize()}: {str(data[cat])}',  html.Br(),])\n",
    "        return construct_string\n",
    "\n",
    "@callback(\n",
    "    Output(\"cytoscape\", \"generateImage\"),\n",
    "    [Input(\"btn-get-jpg\", \"n_clicks\"),\n",
    "        Input(\"btn-get-png\", \"n_clicks\"),\n",
    "        Input(\"btn-get-svg\", \"n_clicks\"),\n",
    "    ])\n",
    "def get_image(get_jpg_clicks, get_png_clicks, get_svg_clicks):\n",
    "    \"Download the graph image.\"\n",
    "    if ctx.triggered:\n",
    "        action = \"download\"\n",
    "        ftype = ctx.triggered_id.split(\"-\")[-1]\n",
    "\n",
    "        return {\n",
    "            'type': ftype,\n",
    "            'action': action\n",
    "            }\n",
    "    else:\n",
    "        return {\n",
    "            'type': 'svg',\n",
    "            'action': 'store'\n",
    "            }\n",
    "\n",
    "# Run the app\n",
    "app.run(debug=True, jupyter_mode='external', port=8052)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989be079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
